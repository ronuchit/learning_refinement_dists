\begin{abstract}
A key challenge in robotics is the execution of long-horizon tasks, which
require the robot to reason through a long sequence of high-level steps in order to reach
a goal, as well as determine appropriate joint trajectories for each step.
Much recent work has been devoted to hierarchical planning, which seeks
to plan for such goals by combining task and motion planning. In this paper, we improve
upon an existing approach that refines symbolic (high-level) plans using an interface layer,
which samples and assigns pose values to the plan's parameters using a
complete backtracking search. A core limitation
of the current framework is the manner in which these pose values are sampled:
using hand-coded constants that leverage specificity about the geometry of the environment
and its objects. To remedy this issue, we apply
techniques in reinforcement learning to train a system to produce good proposal
distributions for sampling pose values. Our primary contribution is a machine learning
approach to finding sampling distributions for the refinement of high-level plans in
a hierarchical planning system. Our secondary contribution is a
novel strategy for refining these high-level plans based on randomization, which we found integrates well
with our learning algorithm. Our experiments demonstrate improvements in motion planning
time and number of motion planner calls in a variety of test environments, when compared
to the existing infrastructure using hand-coded sampling distributions.
\end{abstract}
