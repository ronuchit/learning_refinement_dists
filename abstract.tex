\begin{abstract}
A challenge in mobile manipulation planning is the length of the horizon that must
be considered; it is not uncommon for tasks to require thousands of
individual motions. Planning complexity is exponential in the length
of the plan, rendering direct motion planning intractable for
many problems of interest. Recent work has focused on
\emph{task and motion planning} (TAMP) as a way to address this
challenge. TAMP methods integrate logical search with continuous
geometric reasoning in order to sequence several short-horizon motion plans that together
solve a long-horizon task. To account for continuous parameters, many of these
systems rely on hand-coded discretization of the domain. Such an approach lacks robustness and
requires substantial design effort. In this paper, we present methods to improve the reliability
and speed of planning in a TAMP system. The approach we build on first plans
abstractly, ignoring continuous values, and then performs \emph{plan refinement}
to determine feasible parameter settings. In this paper, we formulate this process as a
Markov decision process (MDP) and give a reinforcement learning (RL) algorithm
to learn a policy for it. We also present intial work that learns which plan,
from a set of potential candidates, to try to refine.
Our contributions are as follows: 1) we present a randomized local search algorithm for plan refinement
that is easily formulable as an MDP; 2) we give an RL algorithm that learns a policy for this MDP;
3) we present a method that trains heuristics for selecting which plan to try to refine;
and 4) we perform experiments to evaluate the performance of our system in a variety of simulated
domains. We find that our approach yields significantly improved performance over that
of hand-coded sampling distributions.
\end{abstract}
