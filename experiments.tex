\section{Experiments}
\subsection{Evaluation}
We evaluate our reinforcement learning system in several pick-and-place
tasks, varying the number of obstructions, obstruction locations, and whether
the base is allowed to move. We compare performance with that of the
hand-coded sampling distributions used in Srivastava et al.~\cite{srivastava2014combined}.
Because our learned distributions are optimized for producing motion planning
feasible samples, we do not evaluate performance on total system
running time; rather, we evaluate the refinement of only the final high level
plan. This is because, if we have a high level plan with \emph{no} valid refinement,
our system may find more IK-feasible samples (and thus trigger more calls to
the motion planner) than the baseline system. This causes increased time spent
on refinement attempts before raising error information back to the task planner,
which should not be penalized. The error propagation occurs when the iteration
limit is reached in the randomized refinement algorithm.

We use the reward function described earlier. Our weight
vectors are initialized to $\vec{\mathbf{0}}$ for all pose parameter types: grasp pose
for each arm, putdown pose for each arm, and base motion.
The initialization represents a uniform distribution across the limits of the geometric search space.
For grasp and putdown actions, the limits are a cube of side length 30 centimeters
around the target object or putdown location. For base motion actions, the limits are a
square of side length 1 meter around the object or location which the robot is approaching.
Currently, our feature function $f$ incorporates only geometric aspects of the sample and environment,
such as distance between the sample and object being grasped, and distance from the sample to
nearby obstructions.

Our experiments are conducted in Python 2.7 using the OpenRave simulator~\cite{Diankov_2008_6117} with a PR2 robot.
The motion planner we use is trajopt~\cite{schulman2013finding}, and the task planner is Fast-Forward~\cite{FF}.
The experiments were carried out in series on an Intel Core i7-4770K machine
with 16GB RAM.

We test five different scenarios, all of which share a goal:
grasp a particular object from a table and put it down at a specific location.
one grasp obstruction, two grasp obstructions, three grasp obstructions,
one grasp obstruction with base motion allowed (this means the system must learn
a distribution for base poses), and putdown location obstructed in the cardinal
directions. Table 1 summarizes our results, and Figure 2 shows the learned distribution
for different iterations in the training process.

\subsection{Discussion}