\section{Experiments}
We evaluate our reinforcement learning system in several pick-and-place
tasks, varying the number of obstructions, obstruction locations, and whether
the base is allowed to move. Because the distributions are optimized for
motion planning successes, we do not evaluate performance on total system
running time; rather, we evaluate the refinement of only the final high level
plan. This is because, if we have a high level plan with no valid refinement,
our system may find more IK-feasible samples (and thus trigger more calls to
the motion planner) than the original system, causing increased time spent
on refinement attempts before raising error information back to the task planner.
The information propagation occurs when the iteration limit is reached in
the randomized refinement algorithm.

We use the following reward function: -1 for an IK-infeasible sample, +3 for
an IK-feasible sample, -3 for action motion planning failure, -3 for action
precondition violation, and +5 for action motion planning success. Our weight
vectors are initialized to 0, representing a uniform distribution across
the limits of the search space. These limits are a cube of side length 30 cm
around an object being grasped or location of putdown, and a square of side
length 1 meter around an object to which the robot is moving.

Our experiments are in Python using the OpenRave simulator~\cite{Diankov_2008_6117} with a PR2 robot.
The motion planner we use is trajopt~\cite{schulman2013finding}, and the task planner is Fast-Forward~\cite{FF}.
The experiments were carried out in series on an Intel Core i7-4770K machine
with 16GB RAM. We test five different scenarios, all of which share the goal
of grasping an object off a table and putting it down at a specific location:
one grasp obstruction, two grasp obstructions, three grasp obstructions,
one grasp obstruction with base motion allowed (this means the system must learn
a distribution for base poses), and putdown location obstructed in the cardinal
directions. Table 1 summarizes our results, and Figure 2 shows the learned distribution
for different iterations in the training process.