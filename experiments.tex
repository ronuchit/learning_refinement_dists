\section{Experiments}
\begin{table}[t]
  \centering
  \vspace{8pt}
  \tabcolsep=0.11cm{
  \begin{tabular}{ccccc}
    \toprule[1.5pt]
      \textbf{Condition} & \textbf{\% Solved (St. Dev.)} & \textbf{Time (sec): A (S)} & \textbf{\# MP Calls: S}\\
    \midrule[2pt]
      30 (can), T & 42 (0) & 179 (6.2) & 8\\
    \midrule
      30 (can), B & 40 (0) & 194 (20.5) & 11\\
    \midrule
      30 (can), L & 72 (8.2) & 132 (20.4) & 11\\
    \midrule
      30 (can), F & 81 (3.0) & 106 (17.9) & 13\\
    \midrule[1.5pt]
      35 (can), T & 50 (0) & 157 (9.2) & 8\\
    \midrule
      35 (can), B & 50 (0) & 161 (17.6) & 9\\
    \midrule
      35 (can), L & 68 (8.3) & 123 (11.6) & 7\\
    \midrule
      35 (can), F & 78 (2.2) & 103 (10.6) & 7\\
    \midrule[1.5pt]
      40 (can), T & 34 (0) & 207 (19.7) & 10\\
    \midrule
      40 (can), B & 36 (0) & 204 (21.7) & 10\\
    \midrule
      40 (can), L & 61 (6.3) & 149 (18.7) & 9\\
    \midrule
      40 (can), F & 74 (3.2) & 128 (20.7) & 10\\
    \midrule[2pt]
    %   2 (dinner) & T & 100 (0) & 35.5 & 60.2\\
    % \midrule
    %   2 (dinner) & B & 100 (0) & 37.3 & 59.2\\
    % \midrule
    %   2 (dinner) & L & 99 (1.8) & 41.5 & 61.6\\
    % \midrule[1.5pt]
      4 (dinner), T & 100 (0) & 43 (43.2) & 98\\
    \midrule
      4 (dinner), B & 90 (0) & 87 (63.0) & 96\\
    \midrule
      4 (dinner), L & 99 (0.6) & 69 (69.2) & 97\\
    \midrule[2pt]
      2 (frying), T & 95 (0) & 48 (29.0) & 67\\
    \midrule
      2 (frying), B & 80 (0) & 98 (46.9) & 60\\
    \midrule
      2 (frying), L & 99 (2.0) & 27 (22.6) & 45\\
    \midrule[1.5pt]
      4 (frying), T & 55 (0) & 176 (48.9) & 132\\
    \midrule
      4 (frying), B & 20 (0) & 278 (187.9) & 156\\
    \midrule
      4 (frying), L & 92 (6.8) & 136 (90.6) & 121\\
    \bottomrule[1.5pt]
  \end{tabular}}
  \caption{\small{Percent solved, standard deviation, average refinement time, and number of motion
      planning calls. A averages across all runs, treating timeouts as 300s; S averages across runs where all systems succeeded.
      T: baseline 1. B: baseline 2.
      L: learned refinement policies with greedy graph search.
      F: learned refinement policies and graph
      search heuristics. L and F results are averaged across 5
      separately trained sets of weights. Time limit: 300s.}}
  \label{table:results}
\end{table}

\subsection{Methodology}
We evaluate our approach in three domains: cans distributed randomly
on a table (the \emph{can domain}), setting up bowls for dinner (the
\emph{dinner domain}), and placing frying pans into a narrow shelf
(the \emph{frying domain}).  We compare performance with two
baselines, both of which use the hand-coded refinement distributions
used in {\sc sfrcra-14}.

Baseline 1 is {\sc sfrcra-14}: it uses exhaustive backtracking search
for refinement and greedy depth-first search of the {\sc prg}, which
always tries to refine the plan that incorporates all error
information obtained thus far.  Baseline 2 uses randomized refinement
with the following fixed graph search policy: try 3 times to refine
the deepest node in the graph; if unsuccessful, generate a geometric
error, replan with the task planner (which creates a child node), and
repeat.

We compare these baselines against two systems. The first combines
learned refinement policies with the graph search used in baseline
2. The second is our full system, and uses learned refinement policies
and graph search heuristics.  For the dinner domain and frying domain,
we focus only on the low-level learning. Since the errors propagated in these
domains only relate to the stackability of objects, a good graph search strategy
incorporates all available error information when
attempting refinement. Thus, the graph search strategy from baseline 2
already performs well.

Initial experimentation revealed that jointly learning weights for all
parameter types was intractable. Thus, we use a curriculum where the
distribution of planning problems, $\Prob$, gets progressively
harder. For the full system, we train the refinement policies first,
then fix them while collecting demonstrations and training the graph
search heuristics. To reduce variance in the process, we train 3 sets
of refinement weights independently and select the one that performs
best on a validation set. We set the step size $\alpha = 0.1$.

We report results on fixed test sets of 50 randomly generated
environments for the can and dinner domains, and 20 for the frying
domain (because these environments have less variation).  For the
third and fourth systems, we average results across running the
training process 5 times independently and evaluating each final set
of weights.

Our features use the concept of a \emph{cone}, parameterized by an apex $a$,
an angle range, a number of buckets, and a point $p$ toward which it is directed.
To bucket a sample $s$ within a cone means to measure the angle between the vector from
$a$ to $s$ and the vector from $a$ to $p$, then determine which bucket of the cone
it lies in, within the angle range. We also denote the closest point on the table edge to an object $o$ as $t_{o}$.

To learn grasping poses for a target object $o$, we use 25 features.
9 binary features bucket the distance between
the sample and $o$. 9 binary features bucket the
sample height. 3 features describe the number of objects within
discs of radius 7, 10, and 15 centimeters around the sample. 3 binary
features bucket the sample within a cone of apex $o$, angle range 0 to $\pi$, and $p$
the robot position. The final feature is binary and is 1 iff the 2-norm of
the difference between the previous sample and the current one exceeds $10^{-3}$.

To learn putdown poses for a target location $l$, we use 24 features. The first
18 are the same as for grasping. 5 binary features bucket the sample within
a cone of apex $l$, angle range 0 to $\pi$, and $p = t_{l}$. The last feature describes
orientation. The final feature is the same as for grasping.

To learn base poses for interacting with an object $o$, we use 18 features. The first 9 are the same as
for grasping. 7 binary features bucket the sample within a cone of apex $o$, angle range 0 to $\pi$, and $p = t_{o}$.
The last feature describes orientation.
1 feature describes if the sampled pose collides with any objects.
1 feature describes whether there are obstructions in front of the sampled pose.

To learn putdown locations $l$, we use 5 features. 1 binary feature encodes whether
any objects are within 15 centimeters of $l$. 1 feature is the
distance from $l$ to the closest table edge. 3 features count the number of objects
in each bucket of a cone with apex $l$, angle range 0 to $\frac{3\pi}{4}$, and $p = t_{l}$.

Our high-level heuristics use features to describe the feasibility of
plans. In our domains, this mainly summarizes issues with potential
grasps. We use three features to evaluate a potential grasp action,
targeted at an object $o$. We consider a cone with apex $o$, angle range
$-\frac{\pi}{3}$ to $\frac{\pi}{3}$, and $p = t_{o}$. The
binary feature exists\_obstr indicates whether any
other objects lie in this cone. The binary feature exists\_path
indicates whether there is a linear grasp path within the cone wide
enough to fit the robot's gripper. The
sweep\_count feature computes the minimum number of collisions with the robot's
gripper placed at any angle inside the cone.
We construct these features for the first five grasp actions in the
plan, padding with -1. We then add on the following
aggregate features associated with the entire plan: 1) the minimum
exists\_obstr across all grasp actions, 2) the sum of sweep\_count
across all grasp actions, 3) the number of times $u$ was picked for
refinement, and 4) the number of times $u$ was picked for generating
an error.

Our experiments are conducted in Python 2.7 using the OpenRave
simulator~\cite{Diankov_2008_6117} with a PR2.  Our motion planner is
trajopt~\cite{schulman2013finding}, and our task planner is
Fast-Forward~\cite{FF}. Experiments were carried out in series on an
Intel Core i7-4770K machine with 16GB RAM. Each problem's time limit
was 300 seconds. Table \ref{table:results} summarizes quantitative
results.

% \begin{figure}[t]
%   \centering
%   \begin{subfigure}[b]{0.35\linewidth}
%     \includegraphics[width=\textwidth]{images/learns.png}
%     \caption{Initial distributions.}
%   \end{subfigure}
%   \begin{subfigure}[b]{0.35\linewidth}
%     \includegraphics[width=\textwidth]{images/learn4.png}
%     \caption{After 4 iterations.}
%   \end{subfigure}
%   \begin{subfigure}[b]{0.35\linewidth}
%     \includegraphics[width=\textwidth]{images/learn8.png}
%     \caption{After 8 iterations.}
%   \end{subfigure}
%   \begin{subfigure}[b]{0.35\linewidth}
%     \includegraphics[width=\textwidth]{images/learn12.png}
%     \caption{Final distributions.}
%   \end{subfigure}
%   \caption{\small{Learned base position (blue) and left arm grasp (green) distributions used to
% pick up the black can after different training iterations for learning refinement policies.
% An iteration refers to a single planning problem,
% which terminates after $H$ calls to the \textsc{Resample} routine.
% Initial distributions are uniform because we initialize weights to $\vec{\mathbf{0}}$.
% Final distributions are after 12 iterations.}}
%   \label{fig:training}
% \end{figure}

\subsection{Can Domain}
We run three experiments, using 30, 35, and 40 cans on the
table.  The goal is always for the robot to pick up a particular can
with its left gripper. We disable the right gripper, so any
obstructions to the target object must be picked up and placed
elsewhere on the table. A typical high-level plan in this domain has around 6 steps.
This domain has 4 types of continuous
references: base poses, object grasping poses, object putdown poses,
and object putdown locations.

Our system trains distributions for base poses and
grasping poses for 12 iterations with $\epsilon = 5$, then base poses,
grasping poses, and putdown poses (at fixed location) for 18
iterations with $\epsilon = 20$, then all reference types for 30
iterations with $\epsilon = 20$. We fix $H = 100$.
To train the graph search heuristics, we collected approximately 300
optimal actions from the human demonstrator, over 3 rounds of dataset aggregation.
After these 3 rounds, performance plateaued. We use $C =
10^{9}$ to solve the maximum-margin optimization problem.

\begin{figure}[t]
  \centering
    \noindent
    \includegraphics[scale=0.13]{images/grasp_context_1.png}
    \includegraphics[scale=0.13]{images/grasp_context_2.png}
    \includegraphics[scale=0.13]{images/grasp_context_3.png}
  \caption{\small{The learned left arm grasping (green) distribution
      for the black can adapts as a single potential obstruction (red) is
      moved.}}
  \label{fig:context}
\end{figure}

The results demonstrate significant improvements in performance when
compared to the baseline systems for success rate and average refinement time. When backtracking
search succeeds, it does so quickly, so it has a faster average
refinement time on the successes. \figref{fig:cover} shows learned
base motion and pickup distributions. \figref{fig:context} shows how
the learned distribution shifts based on the geometric context.

\subsection{Dinner Domain}
We run one experiment, using 4 bowls. The robot must
move the bowls from their initial locations on one table to target
locations on the other. We assign a cost to base motion in the
environment, so the robot is encouraged to use the provided tray, onto
which bowls can be stacked. A typical high-level plan in this domain has around 33 steps.
This domain has 5 types of continuous
references: base poses, object grasping poses, object putdown poses,
tray pickup poses, and tray putdown poses.

Our system first trains base poses and tray pickup
and putdown poses for 20 iterations, then object grasping and putdown
poses for 20 iterations. We fix $H = 100$ and $\epsilon = 10$.

The results demonstrate comparable performance to the baseline
systems. A simple hand-coded discretization works well in
this domain. The optimal robot base pose for tray pickup
is directly in front of it, which is quickly sampled.
The lack of long-term
dependencies in the plan also means that backtracking search succeeds
quickly. This shows that our system recovers the performance of
well-chosen discretizations, when they exist. \figref{fig:cover} shows learned tray
pickup poses after all 20 iterations.

\subsection{Frying Domain}
We run two experiments, using 2 and 4 frying pans. The robot
must stack these pans in order of decreasing radius in a narrow
shelf. To succeed, it must grasp each frying pan at its handle.
A typical high-level plan in this domain has around 45 steps.
This domain has 3 types of continuous references: base poses,
pan grasping poses, and pan putdown poses.

We did not need curriculum learning. We fix $N = 30$, $H = 100$, and
$\epsilon = 5$. {\sc sfrcra-14} did not have a frying domain, so we
use the following hand-coded distribution for picking up the pans: 4
grasping poses in the cardinal directions around the lip of the pan,
and 4 equidistant along the handle.

The results demonstrate significantly higher success rate versus the
baseline systems. Similar to the can domain, when backtracking ``gets lucky'' and
picks grasping poses along the handle early in the search, it succeeds quickly, leading
to a faster average refinement time on the successes.
\figref{fig:cover} shows learned frying pan grasping poses
after all 30 iterations. Our system learned to prefer picking up the
pan at its handle to fit it into the shelf, which is not shown.






