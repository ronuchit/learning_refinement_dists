\section{Experiments}
\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[width=\textwidth]{images/learns.png}
    \caption{Initial distributions.}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[width=\textwidth]{images/learn4.png}
    \caption{After 4 iterations.}
  \end{subfigure}
  \begin{subfigure}[b]{0.35\linewidth}
    \includegraphics[width=\textwidth]{images/learn8.png}
    \caption{After 8 iterations.}
  \end{subfigure}
  \begin{subfigure}[b]{0.35\linewidth}
    \includegraphics[width=\textwidth]{images/learn12.png}
    \caption{Final distributions.}
  \end{subfigure}
  \caption{\small{Learned base position (blue) and left arm grasp (green) distributions used to
pick up the black can after different training iterations for learning refinement policies.
An iteration refers to a single planning problem,
which terminates after $L$ calls to the \textsc{Resample} routine. The
initial distributions are uniform because we initialize weights to $\vec{\mathbf{0}}$.
The final distributions are after 12 iterations.}}
  \label{fig:training}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\linewidth}
    \includegraphics[width=\textwidth]{images/dinner_tray_initial.png}
    \caption{Initial distributions.}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\linewidth}
    \includegraphics[width=\textwidth]{images/dinner_tray_final.png}
    \caption{Final distributions.}
  \end{subfigure}
  \caption{\small{Initial and learned base position (blue) and tray pickup (green, yellow) distributions.
The green points refer to where the right gripper will be placed; the left gripper is
placed in a symmetric position on the other side of the tray, as marked by the yellow points. The
initial distributions are uniform because we initialize weights to $\vec{\mathbf{0}}$.
The final distributions are after 20 iterations.}}
  \label{fig:dinner}
  \vspace{-1.5 em}
\end{figure}

\begin{table}[t]
  \centering
  \vspace{8pt}
  \tabcolsep=0.11cm{
  \begin{tabular}{ccccc}
    \toprule[1.5pt]
      \textbf{\# Objects} & \textbf{System} & \textbf{\% Solved (SD)} & \textbf{Avg Ref Time (s)} & \textbf{Avg \# MP Calls}\\
    \midrule[2pt]
      2 (dinner) & T & 100 (0) & 35.5 & 60.2\\
    \midrule
      2 (dinner) & B & 100 (0) & 37.3 & 59.2\\
    \midrule
      2 (dinner) & L & 99 (1.8) & 41.5 & 61.6\\
    \midrule[1.5pt]
      4 (dinner) & T & 100 (0) & 43.2 & 98.0\\
    \midrule
      4 (dinner) & B & 90 (0) & 63.0 & 95.5\\
    \midrule
      4 (dinner) & L & 99 (0.6) & 69.2 & 97.1\\
    \midrule[1.5pt]
      25 (cans) & T & 74 (0) & 5.4 & 12.7\\
    \midrule
      25 (cans) & B & 76 (0) & 9.7 & 9.7\\
    \midrule
      25 (cans) & L & 92 (4.6) & 12.7 & 10.1\\
    \midrule
      25 (cans) & F & 95 (2.7) & 9.2 & 11.6\\
    \midrule[1.5pt]
      30 (cans) & T & 42 (0) & 6.7 & 9.2\\
    \midrule
      30 (cans) & B & 42 (0) & 27.6 & 15.9\\
    \midrule
      30 (cans) & L & 72 (9.8) & 18.5 & 12.6\\
    \midrule
      30 (cans) & F & 83 (3.0) & 10.2 & 12.8\\
    \bottomrule[1.5pt]
  \end{tabular}}
  \caption{\small{Percent solved and standard deviation, along with time spent refining and number of calls to the motion
planner for baseline 1 (T), baseline 2 (B), our learned refinement policies with the graph search used in baseline 2 (L),
and our full system: learned refinement policies and graph search heuristics (F). Results for L and F
are averaged across 10 separately trained sets of weights. Time limit: 300s.}}
  \label{table:results}
  \vspace{-2 em}
\end{table}

\subsection{Training Methodology}
We use the reward function described earlier. Our weight
vectors are initialized to $\vec{\mathbf{0}}$ for all plan parameter types -- this
initialization represents a uniform distribution across the limits of the geometric search space.
We use 24 features for learning the $\theta_{p}$. 9 binary features encode the bucketed distance between the sample
and target (the object referenced by the parameter). 9 binary features encode the bucketed sample height. 3 features
describe the number of other objects within discs of radius 7, 10, and 15 centimeters around the
sample. 3 binary features describe the angle made between the vector from the
robot to the target and the vector from the sample to the target: whether the angle is less than
$\pi/3$, $\pi/2$, and $3\pi/4$.

Initial experimentation revealed that training weights for all reference types jointly is intractable,
because planning takes a long time. Potential solutions for this would explore alternative RL algorithms,
but this is not our focus. Instead, we apply curriculum learning by training with a planning problem distribution
$\Prob$ that gets progressively harder. Additionally, we train the refinement policies first, then fix them while
training the graph search heuristics.

We evaluate our approach in two distinct domains: cans distributed on a table (the \emph{can domain})
and setting up bowls for dinner (the \emph{dinner domain}).
We compare performance with two baselines, both of which use the hand-coded sampling discretizations for
plan refinement used in SFRCRA-14. Baseline 1 is SFRCRA-14: it uses exhaustive backtracking search for refinement
and greedy depth-first search of the plan refinement graph, which always tries to refine
the plan that incorporates all error information obtained thus far.
Baseline 2 uses randomized refinement with the following fixed graph search policy: try 3 times to refine the deepest
node in the graph; if unsuccessful, generate a geometric fact from it, replan (which creates a child node), and repeat.

For the can domain, we report results for 4 systems: 1) baseline 1; 2) baseline 2; 3) our learned refinement policies
with the graph search used in baseline 2; and 4) our full system, with learned refinement policies and graph search heuristics.
For the dinner domain, we report results only for the first 3 systems, because the errors propagated in this
domain relate to the stackability of objects. Since this is independent of reference instantiations, we want to
incorporate all available error information when attempting refinement. Thus, the graph search strategy
from baseline 2 can be expected to perform well in this setting.

For the fourth system, which trains a refinement policy and graph search heuristics, we employ the following
algorithm to produce a trained set of weights. We train 3 sets independently, test each
one on a validation set of 50 environments, and output the best-performing one. We found that this
reduced variation due to random seeding. For the third system, by contrast, we train a single
set of weights and output it, without any validation.

We report results on a fixed test set of 50 randomly generated environments.
For the third and fourth systems, we average across running the training process 10 times
and evaluating each final set of weights separately.

Our experiments are conducted in Python 2.7 using the OpenRave simulator~\cite{Diankov_2008_6117} with a PR2 robot.
The motion planner we use is trajopt~\cite{schulman2013finding}, and the task planner is Fast-Forward~\cite{FF}.
The experiments were carried out in series on an Intel Core i7-4790K machine with 16GB RAM.
Table \ref{table:results} summarizes our quantitative results. \figref{fig:cover} and \figref{fig:training}
show learned refinement policies for the can domain. \figref{fig:dinner} shows learned tray pickup poses
for the dinner domain.

\subsection{Can Domain}
We run two sets of experiments, using 25 objects and 30 objects on the table.
The goal across all experiments is for the robot to pick up a particular object with its
left gripper. We disabled the right gripper, so any obstructions to the target object must be picked up and
placed elsewhere on the table. This domain has 4 types of continuous references: base poses, object grasp
poses, object putdown poses, and object putdown locations onto the table.
% The range of allowable values for grasp and putdown poses is a cube of side length 30 centimeters around the
% target object or putdown location. For base poses, the range is a square of side length 1 meter around the
% object or location which the robot is approaching. For putdown locations, the range is defined by the table.

Our curriculum learning system first trains base poses and grasp poses for $N = 12$ iterations with $\epsilon = 5$,
then base poses, grasp poses, and putdown poses (at fixed location) for $N = 18$ iterations with $\epsilon = 20$,
then all reference types for $N = 30$ iterations with $\epsilon = 20$. We fixed $L = 100$.

The results demonstrate significant improvements in performance to the baseline systems for success rate.
However, backtracking search provides faster average refinement time. This is likely because the
refinement times were averaged over the test cases where all 4 systems succeeded. These plans tended
to be easier to refine, so an exhaustive backtracking search performs well because the total search space is small.

\subsection{Dinner Domain}
We run two sets of experiments, using 2 and 4 bowls. The robot must move the
bowls from their initial locations on one table to target locations on the other. We assign a cost to
base motion in the environment, so the robot is encouraged to use the provided tray, onto which bowls can be stacked.
This domain has 5 types of continuous references: base poses, object grasp poses, object putdown poses, tray pickup
poses, and tray putdown poses.

Our curriculum learning system first trains base poses and tray pickup and putdown poses for
$N = 20$ iterations, then object grasp and putdown poses for $N = 20$ iterations. We fixed $L = 100$ and $\epsilon = 10$.

The results demonstrate comparable performance to the baseline systems. The reason is that
hand-coded discretizations of the sample space are very good in this domain. For example, the optimal
robot base pose from which to pick up the tray is directly in front of it, which is quickly sampled through
the baseline systems' discretization. Additionally, the lack of long-term dependencies in the plan
means that backtracking search finds a valid refinement quickly. The fact that our system performs comparably
with the baselines shows that our algorithm can learn grasp poses for a variety of objects, such as cylinders
and a tray.