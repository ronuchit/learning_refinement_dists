General
-------

In a lot of places you write "reinforcement learning" or "task and
motion planning" specifically -- this can be somewhat repetitive. Use
RL or TAMP instead.

Abstract
--------

"Long-horizon tasks ... key challenge in the field." --> "A challenge
in mobile manipulation planning is the length of the horizon that must
be considered; it is not uncommon for tasks to requires thousands of
individual motions. Planning complexity is exponential in the length
of the plan, so this renders direct motion planning intractable for
many problems of interest."

"Recent work ... plan refinement" --> "Recent work has focused on
\emph{task and motion planning} (TAMP) as a way to answer this
challenge. These methods integrate logical search and continuous,
geometric reasoning to sequence several short-horizon motion plans to
solve a long-horizon task."

"A core limitation ... " --> "That majority of these approaches rely
on hand-coded discretizations of continuous parameters for motion
planning (e.g., a target grasp pose). These discretizations are often
domain specific and require substantial design effort."

"apply techniques in reinforcement learning..." --> "present a method
that uses reinforcement learning (RL) to learn a distribution that propose
values for the continuous parameters of a high-level plan which are
likely to lead to motion planning success. More specifically, we
formulate \emph{plan refinement}, the process of determining
continuous parameter settings for a fixed task sequence, as a Markov
decision process (MDP) and give an algorithm to learn a policy for that
problem."

"contributions are" --> "contributions are as follows:"

"a novel randomized" --> "we present a randomized" (don't think the
algorithm is that novel...)

"plan refinement" --> "plan refinement, that is well suited to an MDP
formulation."

"a formlation...framework" --> "we give an RL algorithm that learns
policies in the above MDP"

"experiments showing that ... test environments" --> "we performed
experiments to evaluate the performance of this system in a simulation
pick-place domain. We find that our approach achieves performance that
is comparable with that of hand-coded discretizations."


Introduction
------------

"Such robots would face complex tasks..." --> "To be effective, such a
robot will need to perform complex tasks (e.g., setting a table, doing
laundry) over long horizons."

"using only motion planning ... of reasoning" --> "for
state-of-the-art motion planners."

"In this framework, the (classical) task planner produces a symbolic
plan containing a sequence of actions to reach a goal state, and
heuristic sampling techniques propose concrete values for the
continuous variables in the plan, thus grounding it. This process of
assigning candidate values to the plan variables is known as plan
refinement. These candidate values are then checked locally for
feasibility by calling a motion planner.

This hierarchy enforces abstraction between the role of the task
planner and that of the motion planner: the task planner maintains no
knowledge of the environment geometry, and the motion planner has no
understanding of the overall goal. A central challenge in building
such a system is designing good heuristics for the sampling in plan
refinement."

To me this still reads as a description of hybrid planner, rather than
an overview of TAMP approaches. (for example, refinement in HPN adds
logical and continuous parameters, semantic attachments [1] and FFROB
[2] have no refinement at all and just do forward search directly). 

I would move this to largely replace the paragraph describing HP and
lead with the following paragraph:

"One way to approach this challenge relies on combined \emph{task and
motion planning} (TAMP). In this approach, an agent is given a
abstract, logical characterization of actions (e.g., Move, Grasp,
Putdown) as well as a geometric encoding of the
environment. Efficiently integrating these two types of reasoning
(logical and geometric) is challenging and research research has
proposed several methods (citations)."

Then I would follow with two paragraphs

 ~3 sentences describing discretization in these methods

 ~3 sentences describing the issues with this/why it would be good to
 avoid discretization (essentially paragraph 6)

 1 sentence abstractly describing our solution (similar to something from the abstract)
 
"Our work improves directly upon" --> "Our solution builds on"

follow that sentence with a merge of "In this framework ... plan
refinement" and "They propose ... planning module" Include a sentence
along the lines of "While our method is specific to this architecture,
variations of our approach could apply to other TAMP algorithms."

"In this work... parameters" -- This paragraph is the right idea, but
there are some stylistic issues. Change it to have the fllowing
structure:

- introduce RL 
- describe Zhang and Dietterich
- "In our work, we apply this approach to plan refinement in TAMP."
- we use Zucker et al to implement this
- this means we don't need hand-coded solutions

I would replace the contribution list with a (almost) identical copy
of the list from the abstract -- the explanations here are somewhat
redundant (and if they aren't they should be included aboce). At this
point in the introbution the reader should already have a good idea of
what we're doing, the main purpose of this is to outline the paper and
make our contributiosn explicit for reviewers who are lazy.

"comparable to improved performance" -- "our approach is competitive
with hand-coded discretizations with respect to..." move this out of
the list as well.

Related Work
------------

You have enough here on learning for planning; I think what missing is
related work for task and motion planning.

Background
-----------

A
-----

Wrap the MDP definition in a definition environment. You can use the
mdp formulation in http://eecs.berkeley.edu/~dhm/papers/bsp_bbvi.pdf
(start of section 3) as an example. Use \mathcal to distinguish sets
($S$ --> \mathcal{S})

You also need some more details here. Some math to describe the
objective and a definition of a value function should be there as well.

You don't really define RL as a problem here. Reading this, what's the
difference between RL and just solving an MDP? What makes RL hard?
What kinds of approaches do people use? 

B
------
"Zhang and Dietterich ... " -- "Our problem formulation is motivated by Zhang and Dietterich's application of RL to job shop scheduling."

"The goal of job shop scheduling is to" -- "Job shop scheduling is
a combinatorial optimization problem where the goal is to"

new sentence -- "An empirically successful approach to this problem relies on a randomized local search that proposes changes to an existing suboptimal schedule."

"They define the following" --> "They formulate controlling such an
algorithm as an MDP as follows:"

The more you can make this list look like the MDP formulation we use
(sec V-A) the better

\mathcal{S}, \mathcal{A} are sets, not individual states/actions....

"is changing" --> passive voice, rephrase

Its important to mention that they allow infeasible schedules in the
algorithm. 

"They then use...function" --> "They use TD(\lambda) with (linear?)
function approximation to learn a value function for the MDP. 

"used in a one-step lookahead..." -- don't need this (this is just computing pi* from V*)



"find good sampling distributions for motion planning" -- vague,
introduce Zucker et al. by connecting it to our algorithm

mention RRT (e.g., use RL to bias the distribution of a rapidly
exploring random tree)

"They use stochastic gradiant descent" -- I'd describe the algorithm
the way that Zucker does in the paper (IIRC the phrase was "variant of
REINFORCE"?)

Maybe it makes sense to alter the notation so that its clear $\theta$
is the parameter (e.g., $q_\theta(X)$, $q(x; \theta)$)

eq 1 --> f is undefined, needs a period at the end

"They also" --> "They"

eq. 2 --> no mention of epsilon, no dependence of expectation on \theta
eq. 3 --> needs period

"where" --> "Where"

RE: your question on moving the Zucker stuff to background

    yea, you have a point here -- its definitely nicer to see the math
    for our particular system. Maybe just describe their approach at a
    high level here (no math) and refernce the section where we
    include the details.

Include something about the results from Zucker et al?

C
-----

"These are treated as strings" --> delete

", plan: the task planner maintains no knowledge..." --> ", plan." use
an uninformed task planner is proporty of a solution to TAMP, not
TAMP itself!

RE: I'm not sure if now, the Task and Motion Planning subsection of
Background is TOO short (I took out the details about backtracking).

  yes, this is a too short, but not because it doesn't talk about
  backtracking. This section should give some more details on HP I'd
  restructure as follows
   
  - introduce notation for TAMP and define a solution (no real change)
  - many approaches use a hierarchical approach to TAMP
  - introduce HP (as modular approach that uses black-boxes)
  - introduce pose references (as a property of HP not TAMP)
  - define plan refinement, discuss challenges with discretization
  - talk about the interface layer (e.g., returning to the high level)
  - something along the lines of "interested readers are referred to
  (references) for details"
  
"Any continuous variables ... their nature." -- this won't make sense
to anyone who doesn't already know HP

"This process of assigning such" -- awk + passive voice, rephrase

Randomized Refinement
---------------------

First paragraph -- I'd reorganize as follows

 - before we can apply RL we need an MDP formulation of plan refinement
 - we design our approach to imitate Zhang and Dietterich
 - algorithm description + pseudocode references

Maybe make the sampling distributions an argument?

"the high level plan object" --> "a high level plan"

"Line" --> "line" (both locations)

Everywhere "(Line ##)" --> "(l.##)"

"by sampling" --> passive voice rephrase

"For efficiency...." -- we do actually initialize the trajectories
too... they're just initialized as a straightline within the motion
planner. Also, you haven't defined IK. How about "We initialize by
first find bindings for symbolic pose references that satisfy
inverse-kinematics constraints (IK-feasibility). Trajectories are
initialized as straight lines."

"We then call" --> "We call"

"as part of this step"

"Thus, based on ... " -- is this disctintion really correct/important?
what's the difference between a motion planning failure and a
violation of a 'CollisionFree' precondition? 

"We appropriately call" --> "We call"

"one of the parameters" -- not clear what this refers to

"Again for efficiency" -- too informal, also seems redundant -- maybe
1 sentence earlier along the lines of "To avoid unnecessary motion
planning calls, we repeatedly sample robot poses until we find one
that satisfies inverse kinematics."

"We emphasize..." --> "This refinement strategy hos two key
propoerties. The first is a very explicit algorithm state. We show in
the next section that this allows for a straightforward MDP
formulation. We also found that this was beneficial from an
engineering perspective as it allows for easy reproduction of errors
in debugging. Second, it allows parameter..."

Learning Refinement distributions
---------------------------------

"method for training good sampling distributions using reinforcement
learning" --> "a reinforcment learning approach that learns a policy
for plan refinement" (also passive voice!)

"We first describe... " --> "Before describing our algorithm, we show how to formulate the problem as an MDP"

"Then, we describe...refinment" -- delete

A
----

"We note that ... parameter" -- this belongs in a comment after the
list.  

"We note that randomized refinement...its decision" -- I don't think
this is clear. It seems like this says 'randomized refinement defines
how to select p, so we don't learn p.' What we want to say is that
we're focusing on learning how to resample the continuous values
(future work will learn a policy to select p) so we adopt a hand-coded
policy (randomized refinement) for this.

"Under these deinfitions, the transition function is deterministic" --
this doesn't help define the system. You don't need to be as explicit
about this as you are about the actions, states and rewards (i.e., its
ok to say "this is defined implicitly by the motion planning
algorithms" or smth similar)

"Our reward function....feasible values" -- same issue as the actions,
you should just be defining the reward function in the list (1
sentence, 2 tops). The explanation is good, but should be moved to a
seperate paragraph.

"R(a, s')" -- do you mean R(s, a, s')? (Thats how you defined it
earlier)

"We do not provide any bonus reward...valid" -- why not? if this is
necessary in some way, then we need to say why. if not, this just
seems like non-intuitive choice.

"produce more motion planning feasible values" -- rephrase

This may not be possible in time constraints, but one thing that's
missing from the formulation is domain specificity. In our
experiments, we are always using the pick-place domain, so the domain
is the same. But (I think) that we'd want to have a seperate MDP for
pick-place vs laundry vs planning with quadcopters; this means we need
the pddl domain file (but not the problem file) to be a part of the
formulation.

B
----

"We train a weight vector ... high level plan" -- this is a little
confusing. initially it sounds like you have a weight per parameter
(expecially b/c you write $\theta_p$) but then you say its traing for
each paramter type. I also realized that you also don't have types in
your problem definition for TAMP.... Maybe it makes sense to split the
TAMP definition into components a domain (a set of types, a set of
fluents, and a set of operators) and a planning problem (a set of
objects, an initial state, and a goal state). Its important to get
this right, because otherwise this is one of the main details of the
algorithm.

It might make sense to present this by pointing out what is different
from Zucker et al. I think its just that we have different
distributions for different types, so how about the following structure

  - this section show how to adapt Zucker et al. to this setting
  - Zucker et al learns function that maps f(x) to a likelihood
  - this doesn't make any sense in our setting, so we learn 1 set of weights per type



RE: "You mentioned that a lot of things in my "Training Process"
subsection actually belong in the previous subsection, but I intended
for the split to be as follows: A) "Formulation as Reinforcement
Learning: how is this plan refinement stuff actually a MDP/RL
problem?" B) "Training Process: how were things trained, how did you
define your weights, etc?" I think the current draft of the paper
handles this split effectively."

   That's a sensible split, but I don't think that's reflected in this
   draft. For example, the initial state distribution $\mathcal{P}$ is
   a property of the MDP, not the training algorithm. The same goes
   for $L$ (which defines the transition distribution) and
   $R$. Describing the features here makes sense, but IMO the others
   are out of place.

The description of the training algorithm is hard to follow. Maybe
split it into two parts:

1) executing a policy (we use MCMC to sample + keep a history of (f_t, r_t) tuples)
2) updating your policy (given a history of features and rewards what do we do

It also might be a good idea to put in some pseudocode for this.

"as the system tasks on-policy actions for variable resampling" --
awkward, rephrase 

"If a valid, motion planning feasible refinement is found we forcibly
resample one of the high level variables at random (potentially making
the refinement infeasible again) and continue" -- This is a little
confusing to me, but I don't think its an attribut of the training
algorithm -- it seems to me that this is defining the transition
distribution of the MDP we're solving. But we don't do this at test
time (and wouldn't want to) so this seems like an error to me.

As a side note, the term "motion planning feasible" should be defined
somewhere (its a phrase you and I use a lot, but I don't think its
fair to expect that readers will be familiar with it).

"We sample x from q using the Metropolis algorithm ... " -- I don't
think we want to explain MCMC, we can just reference it (citation
needed) and let people who aren't familiar look it up.

"This process evinces the strength" -- awk, rephrase

"The distribution q ... linear function approximation" -- its hard for
me to tell what this paragraph is conveying. If you're defining what
we're learning I think that should go at the start of the section.

Experiments
------------

A
---

"Because our learned distributions..." -- I think there are cleaner
ways to justify this evaluation metric. I think the level of detail
here will be confusing for some people.

  - in this work, we focus on the problem of refining a single hlp
  - however, during actual planning, we usually start with an HLP that
    has no refinement
  - in future work, we plan to incorporate this into learning, but for
    now we measure performance only on refining the HLP that is
    actually correct.

"For grasp and putdown actions... approaching" -- earlier you said
that parameter values are grouped by type, but now you're saying that
they're grouped by action. It would be good to be explicit here: "Our
domain has 3 types of continuous paramters: .... The range of allowable
values are...."

"Currently our feature function...." -- this needs much more detail,
there should be enough here for someone to reproduce this. "We have 20
features. 10 binary features that encode ...."

When we talk about the experiments, it might make sense to interleave
some of the analysis and description. We have 3 experiments

1) no base motion pick obstructed (1-3 obj)

2) no base motion putdown obstructed -- mention up front that this is
a contrived example to illustrate the issues with coarse discretization

3) base motion + pick/putdown

I think it makes sense to introduce each of these on their own and
then follow up with some analysis for that case. Some observations
that might be useful to discuss
  
   - L does very well with 1 and 3 obstructions but worse with 2. Is
     there an interesting explanation?

   - # MP call and Avg MP time don't always seem to have a linear
       relationship, any reason for that?

   - B fails exactly 33.33 percent -- why's this?

   - L does worse that B when we include base motion (scenario 5) --
     why is that?  is this a meaningful difference?

B
---

I think it makes sense to integrate the analysis and discussion into
the above section rather than separating it out like this.

"illuminating its lack of robustness" -- seems a little over the top

Conclusion
----------------

"is improving" -- passive voice

"Another is traing a system that decides ... " -- passive voice, I
also think its a little hard to understand. It might be easier to
describe this as a change to the policy class.

"optimize more directly for producing a valid refinement" -- not sure
I buy this. Its not really a refinement policy any more, but
meta-level control for hierarchical search.

Figures
----------

As a general comment, the coordinate frame (red/green/blue corner) is
hard to see and interpret here. It might be worth it to use a
different icon (points would probably convey the same info) -- this
would let you color code the distributions (like in Fig. 1 where you
have grasp, putdown, and BP in the same fig). If you want direction,
you might be able to plot arrows.

Choice of figures looks good otherwise.

Fig. 1

  "in our experimental setup" --> "in a simulated pick-place domain"
  include a 1 sentence summary of our system (for the teaser picture
  its good to make it stand along in case someone starts there). 

  "We sample from ... hand-coded ones" --> "These distributions are
  optimized to reduce the number of motion planning calls needed to
  compuate a plan."

Fig. 2

  Caption needs 1 sentence of analysis (what is the reader supposed to
  deduce from this)

Fig. 3
  
  If its not too much trouble, regenerating this one (so that the
  cylinders don't overlap) might be a good idea. 

  "The final distribution from Figure 2" --> "Our learned grasp pose distribution"


[1] Christian Dornhege, Patrick Eyerich, Thomas Keller, Sebastian Tru Ìˆg, Michael Brenner, and Bernhard Nebel. Semantic attachments for domain-independent planning systems. ICAPS 2009
[2] Don't have a directly citation handy -- google search for <garret FFROB kaelbline> should find it; Chris has the reference if you can't find it
