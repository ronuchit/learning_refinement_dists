\section{Related Work}
Dearden and Burbridge~\cite{deardenplanningtamp}
learn probability models that map symbolic predicates to
geometric states in {\sc tamp} problems. They use these models in a
backtracking search over potential geometric state instantiations for
the sequence of symbolic states visited by the plan.  Their work
differs from ours in two key ways. First, they focus on learning the
semantics of symbolic predicates, whereas we assume known semantics
and instead optimize for fast planning. Second, they sample geometric
states independently for each symbolic state in the task sequence,
while our distributions are conditioned on an entire plan and its
current refinement.

One set of approaches to {\sc tamp} involves search through a discretized space
of high-level plans. Kaelbling et al.~\cite{kaelbling2011hierarchical} search
through a hierarchy of these spaces and use ``geometric suggesters''
to lazily perform this discretization. Dornhege et
al.~\cite{dornhege2012semantic} define a way to integrate this
discretization in classical task planning. Garrett et
al.~\cite{GarrettWAFR14} define a heuristic for {\sc tamp} that allows
the search to incorporate geometric feasibility. Our approach could
augment these systems by learning domain-specific discretizations of
parameters.

An alternative set of approaches treats the high-level plan as defining a
constraint satisfaction problem ({\sc csp}) where the parameters of the plan
are the variables of the {\sc csp}. Lozano-P{\'e}rez and
Kaelbling~\cite{lozano2014constraint} discretize the parameter space
and use a standard {\sc csp} solver. Lagriffoul et
al.~\cite{lagriffoul2014orientation} and Garrett et
al.~\cite{garrett2015backward} use a set of relaxed constraints to
reduce the search space for a global
solution. Toussaint~\cite{toussaint2015logic} treats the problem as a
global trajectory optimization. Our methods could be used to learn
domain-specific initialization or discretization strategies for these
approaches.

Our problem formulation is motivated by Zhang and Dietterich's
application of {\sc rl} to job-shop
scheduling~\cite{JobShopSched}. Job-shop scheduling is a combinatorial
optimization problem where the goal is to find the minimum-duration
schedule for a set of jobs with temporal and resource constraints. An
empirically successful approach relies on a randomized
local search that proposes changes to an existing suboptimal
schedule. The authors formulate this as an {\sc mdp} and use {\sc
  td}($\lambda$)~\cite{suttonbarto} with function approximation to
learn a value function. Their approach outperformed the
previous state-of-the-art and scaled better to larger scheduling problems.

Zucker et al.~\cite{workspacebias} use {\sc rl} to bias the
distribution of a rapidly exploring random tree ({\sc rrt}) for motion
planning. They use features of a discretized workspace to train a
non-uniform configuration space sampler with a policy gradient
algorithm. Our work applies similar ideas to {\sc tamp}.

% Another line of research uses machine learning to learn heuristics for
% search. This general formulation is applied to many domains other than
% robotics.  Boyan et al.~\cite{Boyanlearning} solve optimization
% problems by learning a state evaluation function that guides local
% search. Arbelaez et al.~\cite{hamadisearch} solve constraint
% satisfaction problems using a heuristic model that is refined with
% supervised learning. Xu et al.~\cite{Xu07discriminativelearning}
% train heuristics to control forward state-space beam search in task
% planning.
