\section{Related Work}
Dearden and Burbridge~\cite{deardenplanningtamp} use machine learning for {\sc tamp} by learning probability
models that map symbolic predicates to geometric states. They use these models in a backtracking search
over potential geometric state instantiations for the sequence of symbolic states visited by the plan.
Their work differs from ours in two key ways. First, they focus on learning the semantics of symbolic predicates, whereas
we assume known semantics and instead optimize for fast planning. Second, they sample geometric states independently
for each symbolic state in the task sequence, while our distributions are conditioned on the
entire plan and its current refinement.

Kaelbling et al.~\cite{kaelbling2011hierarchical} use hand-coded ``geometric suggesters'' to propose
continuous geometric values for the plan parameters. These suggesters are heuristic
computations which map information about the robot type and geometric operators to a restricted
set of values to sample for each plan parameter. Our methods could be adapted here to learn these
suggesters.

Lagriffoul et al.~\cite{lagriffoul2014orientation}
propose a set of geometric constraints that involve the kinematics and sizes of the specific objects of
interest in the environment. These constraints then define a feasible region from which to search
for geometric instantiations of plan parameters. By contrast, our approach learns good geometric values
to propose based on experience.

Garrett et al.~\cite{GarrettWAFR14} use information about reachability in the configuration
space and symbolic state space to construct a \emph{relaxed plan graph} that guides motion
planning, using geometric biases to break ties. Intermediate poses are sampled from a hand-coded
distribution, whereas we learn these distributions using {\sc rl} and could use them with this graph.

Our problem formulation is motivated by Zhang and Dietterich's application of {\sc rl} to job
shop scheduling~\cite{JobShopSched}. Job shop scheduling is a combinatorial optimization problem where the goal is to find
a minimum-duration schedule of a set of jobs with temporal and resource constraints. An empirically
successful approach to this problem relies on a randomized local search that proposes changes to an
existing suboptimal schedule. The authors formulate this as an {\sc mdp} and use {\sc td}($\lambda$)~\cite{suttonbarto} with function
approximation to learn a value function for it. Their approach outperforms the previous state of the art for this task and
scales better to larger scheduling problems.

Zucker et al.~\cite{workspacebias} use {\sc rl} to bias the distribution of a rapidly exploring random tree ({\sc rrt})
for motion planning. They use features of a discretization of the workspace to train
a non-uniform configuration space sampler using policy gradient algorithms.
In our work, we adapt their gradient updates to the {\sc tamp} framework (Section V-C).

Another line of research has been devoted to machine learning techniques that
guide search algorithms. This general formulation is applied to many domains other than robotics.
Boyan et al.~\cite{Boyanlearning} solve optimization problems by learning a state
evaluation function that guides local search. Arbelaez et al.~\cite{hamadisearch} solve constraint
satisfaction problems using a heuristic model that is refined through supervised learning.
Xu et al.~\cite{Xu07discriminativelearning} train heuristics for controlling forward
state-space beam search in classical task planners.

% Boyan et al.~\cite{Boyanlearning} present an application to solving optimization
% problems using local search routines. They describe an algorithm, \textsc{Stage},
% for learning a state evaluation function using features of the optimization problem.
% This function then guides the local search toward better optima.

% Arbelaez et al.~\cite{hamadisearch} use machine learning to solve constraint
% satisfaction problems (CSPs). Their approach, Continuous Search, maintains a heuristic
% model for solving CSPs and alternates between two modes. In the \emph{exploitation} mode,
% the current heuristic model is used to solve user-inputted CSPs. In the \emph{exploration}
% mode, this model is refined using supervised learning with a linear SVM.

% For instance, Xu et al.~\cite{Xu07discriminativelearning} apply machine learning for classical task
% planning, drawing inspiration from advances in discriminative learning for
% structured output classification. Their system trains heuristics for controlling forward
% state-space beam search in task planners.
