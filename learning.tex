\section{Learning Refinement Distributions}
In this section, we present our primary contribution: a method
for training good sampling distributions using reinforcement learning.
We first describe how to formulate plan refinement in the reinforcement
learning framework. Then, we describe the training process and how to learn
a policy for plan refinement.

\subsection{Formulation as Reinforcement Learning}
We formulate plan refinement as an MDP as follows:
\begin{tightlist}
\item A state $s \in S$ is a tuple $(P, r_{cur}, E)$, consisting of the
symbolic plan, its current refinement (instantiation of values for all plan parameters),
and the environment.
\item An action $a \in A$ is a pair $(p, x)$, where $p$ is the discrete-space plan
parameter to resample and $x$ is the continuous-space value to assign to $p$.
We note that randomized refinement defines a fixed policy for
selecting $p$, using motion planning failure and precondition violation information
to guide its decision. Thus, we focus on training a policy for choosing $x$,
by defining a sampling distribution (based on the state) for each plan parameter.
\item Under these definitions, the transition function is deterministic. 
\item Our reward function $R(a, s')$ provides constant rewards based on parameter resampling.
Because we check for IK feasibility when resampling, we provide $-1$ reward every
time we sample an IK-infeasible pose, and $+3$ reward for an IK-feasible pose.
Similarly, we give $-3$ reward upon motion planning failure or action precondition violation,
and $+5$ reward upon a successful call to the motion planner for an action. We do not
provide any bonus reward for being in a state where the entire plan refinement is valid.
This choice of $R$ implies that our training optimizes the sampling distributions for
motion planning feasibility, rather than explicitly for refining plans. However, the two are
closely correlated -- randomized refinement succeeds in fewer iterations when the sampling
distributions produce more motion planning feasible values.
\end{tightlist}

\subsection{Training Process}
We now show how to apply the method of Zucker et al.~\cite{workspacebias} to learn a policy
for plan refinement. We train a weight vector $\theta_{p}$, associated with the
plan parameter $p$, for each parameter \emph{type} (e.g. ``left gripper grasp pose'' or ``right
gripper putdown pose''), so that the learned distributions are not tied to any single
high level plan. These weight vectors encode the policy for selecting $x$ for actions $a \in A$,
as described in the previous subsection. Our feature function $f(s, p, x)$ maps the current
state $s \in S$, plan parameter $p$, and sampled value $x$ for $p$ to a feature vector; $f$
defines a policy class for our problem.

Our training system is defined by the following:
\begin{tightlist}
\item[$\Prob$:] A distribution over planning problems, encompassing both
the task planning problem and the environment.
\item[$\theta_{p}$:] Described above.
\item[$R$:] The reward function, described in the previous subsection.
\item[$f(s, p, x)$:] Described above.
\item[$N$:] The number of planning problems on which to train.
\item[$L$:] The number of samples for one planning problem.
\item[$\epsilon$:] The number of samples comprising an episode. We perform
a weight vector update after each episode.
\end{tightlist}

The training is a natural extension of randomized
refinement and progresses as follows. $N$ times, sample from $\Prob$ to obtain
a complete planning problem $\Pi$. For each $\Pi$, run the randomized refinement
algorithm to attempt to find a valid plan refinement, allowing the \texttt{resample}
routine to be called $L$ times. As the system takes on-policy actions $a \in A$ (based on
current $\theta$ values) for variable resampling, collected
rewards are appended to a global reward list. If a valid, motion planning
feasible refinement is found, we forcibly resample one of
the high level variables at random (potentially making the refinement infeasible again)
and continue. Every $\epsilon$ calls to
\texttt{resample}, we perform a gradient update on the weight vectors $\theta$ using the
collected rewards and samples since the previous update.

This process evinces the strength of randomized refinement for our system: only the variable
instantiations responsible for failures are resampled, so the learning is naturally
focused toward improving sampling distributions which are not yet well-shaped.

We adopt the distribution in (1) for a parameter $p$ with sample value $x$,
in state $s \in S$:
\begin{equation}
q(s, p, x) \propto exp(\theta_{p}^{T} f(s, p, x))
\end{equation}
and update the $\theta_{p}$ according to (2) - (4).

We sample $x$ from $q$ using the Metropolis algorithm,
an MCMC technique for sampling from a probability distribution using
a symmetric proposal distribution. At each iteration, the sampled value
is based only on the previous one, so the samples form a Markov chain.
Also, since our distributions are continuous, we cannot easily calculate $E_{q}[f]$,
so we approximate it by averaging together the feature vectors for several samples from $q$.

The distribution $q$ represents the policy we are training: it describes how to
sample a continuous-space value $x$ for a parameter $p$, given current state $s$.
Since we learn weights for a linear combination of the features, we have employed
policy gradient techniques with linear function approximation.