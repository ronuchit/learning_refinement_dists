\section{Learning Refinement Distributions}
In this section, we present our primary contribution: an RL
approach that learns a policy for plan refinement. Before describing our
algorithm, we show how to formulate the problem as an MDP.

\subsection{Formulation as Reinforcement Learning Problem}
We formulate plan refinement as an MDP as follows:
\begin{tightlist}
\item A state $s \in \St$ is a tuple $(P, r_{cur}, E)$, consisting of the
symbolic plan, its current refinement (instantiation of values for all plan parameters),
and the environment.
\item An action $a \in \A$ is a pair $(p, x)$, where $p$ is the discrete-space plan
parameter to resample and $x$ is the continuous-space value assigned to $p$ in the new refinement.
\item The transition function is defined implicitly by the motion planning algorithms we use as a black box.
\item The reward function $R(s, a, s')$ provides rewards based on closeness to a valid plan refinement.
\item $\Prob$, a distribution over planning problems, encompassing both the task planning problem
and the environment. This defines the initial state distribution for the MDP.
\item $L$, the number of samples for one planning problem, which partially defines the
transition function for the MDP. (After $L$ calls to the sampler, the next state is drawn uniformly
at random from $\Prob$.)
\end{tightlist}

We are focused on training policies for choosing the continuous value
$x$ in actions $a \in \A$, by defining a sampling distribution (based on the state) for each plan parameter $p$.
We note that randomized refinement provides a fixed policy for selecting $p$ itself.

Our reward function $R$ explicitly encourages successful plan refinement, providing positive reward linearly
interpolated between 0 and 20 based on the fraction of the plan variables whose current instantiation causes
action preconditions to be met. Additionally, we give $-1$ reward every time we sample an IK infeasible pose,
to minimize how long the system spends resampling plan variables until obtaining IK feasible samples.

\subsection{Training Process}
We learn a policy for this MDP by adapting the method of Zucker et al.~\cite{workspacebias}, which
learns a mapping from a feature vector to a likelihood. In our setting, we learn a weight
vector $\theta_{p}$ for each parameter \emph{type}, comprised of a pose type and a gripper
(e.g., ``left gripper grasp pose,'' ``right gripper putdown pose''). This decouples the learned
distributions from any single high level plan and allows us to test in more complicated environments
than those used in training. These weight vectors encode the policy for selecting $x$ in actions $a \in \A$.

We develop a feature function $f(s, p, x)$ that maps the current
state $s \in \St$, plan parameter $p$, and sampled value $x$ for $p$ to a
feature vector; $f$ defines a policy class for the MDP. Additionally, we define
$N$ as the number of planning problems on which to train, and
$\epsilon$ as the number of samples comprising a training episode.

The training is a natural extension of randomized
refinement and progresses as follows. $N$ times, sample from $\Prob$ to obtain
a complete planning problem $\Pi$. For each $\Pi$, run the randomized refinement
algorithm to attempt to find a valid plan refinement, allowing the \texttt{resample}
routine to be called $L$ times before termination. As the system takes on-policy actions $a \in \A$ (based on
current $\theta$), collect rewards according to $R$. After every $\epsilon$ calls to
\texttt{resample}, perform a gradient update on the weight vectors using the
collected rewards.

This process shows the strength of randomized refinement: only the variable
values responsible for failures are resampled, so the learning is naturally
focused toward improving sampling distributions which are not yet well-shaped.

\subsection{Distribution and Gradient Updates}
We adopt the sampling distribution used in Zucker et al.~\cite{workspacebias}
for a parameter $p$ with sample value $x$, in state $s \in \St$:
$$q(s, p, x) \propto exp(\theta_{p}^{T} f(s, p, x)).$$
The authors define the expected reward of an episode $\xi$:
$$\eta(\theta_{p}) = \mathbb{E}_{q}[R(\xi)]$$ and provide an approximation for its gradient:
$$\nabla \eta(\theta_{p}) \approx \frac{R(\xi)}{\epsilon} \sum_{i=1}^{\epsilon}(f(s, p, x_{i}) - \mathbb{E}_{q,s}[f]).$$
$R(\xi)$ is the sum over all rewards obtained throughout $\xi$, and
$\mathbb{E}_{q,s}[f]$ is the expected feature vector under $q$, in state $s \in \St$. The weight vector update is then:
$$\theta_{p} \leftarrow \theta_{p} + \alpha \nabla \eta(\theta_{p})$$
for appropriate step size $\alpha$.

We sample $x$ from $q$ using the Metropolis algorithm~\cite{chib1995understanding}.
Since our distributions are continuous, we cannot easily calculate $\mathbb{E}_{q}[f]$,
so we approximate it by averaging together the feature vectors for several samples from $q$.