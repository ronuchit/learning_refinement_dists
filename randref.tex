\section{Randomized Refinement}
Before we can apply RL to plan refinement, we must formulate it as an MDP.
We design our approach to imitate that of Zhang and Dietterich~\cite{JobShopSched}, by
initializing an infeasible refinement and continually making local improvements.
We thus present one contribution of our work: randomized refinement, a local search
algorithm for plan refinement. It maintains at all times a value for each high level plan
variable; at each iteration, a variable whose current value is leading to a motion planning failure or
action precondition violation is picked randomly and resampled (locally).
Algorithm \ref{alg-randref} shows pseudocode for randomized refinement.

The procedure takes two arguments, a high level plan and a maximum
iteration count. In line 2, we initialize all variables in the high level plan by sampling
from their corresponding distributions. We continue sampling
until we find bindings for symbolic pose references that satisfy
inverse kinematics constraints (IK feasibility). Trajectories are then
initialized as straight lines.

\begin{algorithm}
 \caption{Randomized refinement.} \label{alg-randref}
 \begin{algorithmic}[1]
  \Procedure{RandRef}{$HLP, N$}
  \State $init \leftarrow$ initRefinement($HLP$)
  \For {iter = 0, 1, ..., $N$}
  \State $failStep, failPred \leftarrow $\textsc{MotionPlan}($HLP$)
  \If {$failStep$ is null}
  \State \Return success
  \EndIf
  \If {$failPred$ is null}
  \State \# \emph{Motion planning failure.}
  \State $failAction \leftarrow HLP.ops[failStep]$
  \State resample($failAction.params$)
  \Else
  \State \# \emph{Action precondition violation.}
  \State resample($failPred.params$)
  \EndIf
  \EndFor
  \State Raise failure to task planner, receive new plan.
  \EndProcedure

  \Procedure{MotionPlan}{$HLP$}
  \For {$op_{i}$ in $HLP.ops$}
  \State $res \leftarrow op_{i}$.motionPlan()
  \If {$res.failed$}
  \State \Return $i$, null
  \EndIf
  \State $failPred \leftarrow op_{i}$.checkPreconds()
  \If {$failPred$ is not null}
  \State \Return $i$, $failPred$
  \EndIf
  \EndFor
  \State \# \emph{Found successful plan refinement.}
  \State \Return null, null
  \EndProcedure
 \end{algorithmic}
\end{algorithm}

We call the \textsc{MotionPlan} subroutine in line 4, which
iterates through the sequence of actions that comprise the high level plan (l.20).
For each action, a call to the motion planner is made (l.22) using the instantiated values
for the parameters of that action, to attempt to find a trajectory
linking the sampled poses. If this succeeds, the preconditions of the action
are tested (l.26); as part of this step, we check if the trajectory returned by the
motion planner is collision-free.

We call the \texttt{resample} routine on the high level parameters
associated with a failure; this routine picks one of these high level parameters at random and
resamples it from its distribution. If we reach the iteration limit (l.18),
we convert the most recent failure information into a symbolic representation, then raise it
to the task planner, which will update its fluent state and provide a new
high level plan.

Randomized refinement has two key properties. The first is a very explicit algorithm state.
We show in the next section that this allows for a straightforward MDP
formulation. This is also beneficial from an
engineering perspective, as the simplicity allows for easy debugging. The second is that
it allows the parameter instantiations for a particular action in
the plan to be influenced by those for a \emph{future} action. For example, in a
pick-and-place task, it can make sense for the object's grasp pose to be sampled
conditionally on the current instantiation of the putdown pose, even though the putdown
appears after the grasp in the plan sequence. Thus, it is easy for plan refinement to
respond to long-term dependencies in continuous values of plan variables.