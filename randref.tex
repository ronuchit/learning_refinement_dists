\subsection{A Randomized Algorithm for Plan Refinement}
In order to apply the complete planning algorithm described above, we must provide definitions for each of the subroutines mentioned in Alg.\,\ref{alg:complete}. There are many ways to implement these functions. For example, SFCRA-14 uses a backtracking search over a discrete set of instantiations to implement \textsc{RefineNode}. Since we want to apply RL to learn policies for refinement,
we seek an algorithm that allows for easy formulation as an MDP. Our method
imitates that of Zhang and Dietterich~\cite{JobShopSched}:
we initialize an infeasible refinement and use a randomized local search to propose
improvements. Alg.\,\ref{alg:randref} shows pseudocode for this refinement strategy,
which implements \textsc{RefineNode} in Alg.\,\ref{alg:complete}. The main difference between our implementation and the pseudocode in Alg.\,\ref{alg:complete} is that we use information about previous failed motion planning attempts to guide selection of the next instantiation.

The algorithm takes as input a high-level plan and a maximum iteration count.
In line 1, we initialize a (potentially invalid) refinement by sampling from distributions associated
with each symbolic reference. We continue sampling
until we find bindings that satisfy inverse kinematics constraints (IK feasibility). Trajectories are
initialized as straight lines.

\begin{algorithm}[t]
\begin{small}
  \SetAlgoLined
  \DontPrintSemicolon
  \SetKwProg{myalg}{Algorithm}{}{}
  \SetKwProg{myproc}{Subroutine}{}{}
  \myalg{RandRef($HLP, N_{max}$)} {
  \nl $init \leftarrow$ \textsc{InitRefinement}($HLP$)\;
  \nl \For {iter = 0, 1, ..., $N_{max}$} {
  \nl $failStep, failPred \leftarrow $\textsc{MotionPlan}($HLP$)\;
  \nl \If {$failStep$ == NULL} {
  \tcc{\footnotesize Found valid plan refinement.}
  \nl return success }
  \nl \ElseIf {$failPred$ == NULL} {
  \tcc{\footnotesize Motion planning failure.}
  \nl $failAction \leftarrow HLP.ops[failStep]$\;
  \nl \textsc{Resample}($failAction.params$) }
  \nl \Else {
  \tcc{\footnotesize Action precondition violation.}
  \nl \textsc{Resample}($failPred.params$) } }}

\end{small}
\caption{Randomized local search for plan refinement.}
\label{alg:randref}
\end{algorithm}

The \textsc{MotionPlan} subroutine called in line 3 attempts to
find a collision-free set of trajectories linking all pose instantiations.
To do so, it iterates through the sequence of actions that comprise the high-level plan.
For each, it first calls the motion planner to find a trajectory
linking the sampled poses. If this succeeds, it tests the action preconditions;
as part of this step, it checks that the trajectory is collision-free.

We then call the \textsc{Resample} routine on the symbolic parameters
associated with the infeasibility; this routine picks one at random and
resamples its value. \textsc{InitRefinement} and \textsc{Resample} together define
\textsc{NDGetInstantiation} for our implementation, while \textsc{GetError} iterates
through the steps of the plan, checks precondition and trajectory feasibility, and returns
a failed action index and associated predicate. 

Randomized refinement has two key properties. The first is a very explicit algorithm state.
We show in the next section that this allows for a straightforward MDP
formulation. This is also beneficial from an
engineering perspective, as the simplicity allows for easy debugging. The second is that
it allows the instantiations for a particular action in
the plan to be influenced by those for a \emph{future} action. For example, in a
pick-and-place task, it can make sense for the object's grasp pose to be sampled
conditionally on the current instantiation of the putdown pose, even though the putdown
appears after the grasp in the plan sequence. This allows plan refinement to account for
long-term dependencies in the instantiation of symbolic references.