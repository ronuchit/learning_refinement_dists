\section{Solving Task and Motion Planning Problems}
Solving {\sc tamp} problems requires evaluation of possible courses of
action, each comprised of different combinations of instantiated
action operators. A fundamental challenge is that the set of possible
action instantiations is infinite.  We give a brief overview of {\sc
  sfrcra-14}, a recent approach to {\sc tamp}, and refer the
interested reader to the cited paper for further details.  Then, we
present a complete algorithm for {\sc tamp} that uses the framework of
{\sc sfrcra-14}.

\subsection{Preliminaries}
The fundamental {\sc tamp} problem is that high-level logical
descriptions are lossy abstractions of the true environment
dynamics. Thus, they may not include sufficient information to
determine the applicability of a sequence of actions.  {\sc sfrcra-14}
addresses this by: incrementally searching for a high-level plan that
solves the logical abstraction of the given {\sc tamp} problem;
determining a prefix of the plan that has a motion planning feasible
refinement; updating the high-level abstraction to reflect the reason
for infeasibility; and searching for a new plan suffix from the
failure step onwards.

In general, including geometric properties in the logic-based
formulation leads to an increase in the number of objects representing
distinct poses and/or trajectories. For instance, expressing the fact
that a trajectory for grasping \emph{can$_1$} is obstructed by
\emph{can$_3$} from the current pose of the robot may require
setting a fluent of the form \emph{obstructs(can$_3$, pose$_{17877}$,
  trajectory$_{3219}$, can$_1$)} to true in the description of the
high-level state. In turn, this would require adding
\emph{pose$_{17877}$} and \emph{trajectory$_{3219}$} into the set of
objects if they were not already included. Unfortunately, the size of
the abstracted, logic-based state space grows exponentially with the
number of objects, and such an approach quickly leads to unsolvable
task planning problems.

{\sc sfrcra-14} addresses this challenge by abstracting the continuous
action arguments, such as robot grasping poses and trajectories, into
a \emph{bounded} set of symbolic references to potential values. A
\emph{high-level} plan refers to the fixed task
sequence returned by a task planner; it is comprised of these symbolic
references. An \emph{interface layer} conducts plan refinement,
searching for instantiations of continuous values for symbolic
references while ensuring action feasibility.  The resulting process
is able to utilize off-the-shelf task and motion planners while
carrying out the necessary exchange of information in a scalable
manner.

However, this algorithm has two
main limitations: 1) it is not guaranteed to find a solution when
there exists one, and 2) instantiations
are sampled from object-specific, hand-coded distributions. Since the algorithm
never reduces the set of possible sampled values, its
efficiency degrades as the number of symbolic references increases. In the next subsection,
we address the first limitation; in the following sections, we address the second.

\begin{figure}[t]
  \centering
  \begin{tikzpicture}[sibling distance=10em,
  every node/.style = {shape=rectangle, rounded corners,
    draw, align=left,
    color=black},
  decoration={
      markings,
      mark=at position 1 with {\arrow[scale=2,black]{latex}};
    }]
  \node (1) at (-1, 0) {1. \emph{MoveTo}(pose$_{init}$, pose$_{A}$)\\2. \emph{Grasp}($A$, pose$_{A}$)};
  \node (2) at (-3, -3) {1. \emph{MoveTo}(pose$_{init}$, pose$_{B}$)\\2. \emph{Grasp}($B$, pose$_{B}$)\\
    3. \emph{MoveTo}(pose$_{B}$, pose$_{A}$)\\4. \emph{Grasp}($A$, pose$_{A}$)};
  \node (3) at (1.5, -3) {1. \emph{MoveTo}(pose$_{init}$, pose$_{C}$)\\2. \emph{Grasp}($C$, pose$_{C}$)\\
    3. \emph{MoveTo}(pose$_{C}$, pose$_{A}$)\\4. \emph{Grasp}($A$, pose$_{A}$)};
  \draw [postaction={decorate}] (-2, -0.5) -- (2) node [midway, left, draw=none, text=red, align=center] {B obstructs\\path to A};
  \draw [postaction={decorate}] (0, -0.5) -- (3) node [midway, right, draw=none, text=red, align=center] {C obstructs\\path to A};
  \end{tikzpicture}
  \caption{\small{A plan refinement graph for an environment with 3
      objects: A, B, and C. The goal is to grasp A. Each node is a
      high-level plan, and edges are labeled with errors discovered during
      failed attempts at plan refinement. Each edge points to a plan
      that addresses that reason for failure. In this example, failures
      are obstructions and new plans move the offending object out of
      the way.}}
  \label{fig:prg}
\end{figure}

\subsection{A Complete Algorithm for {\sc tamp}}
We introduce a complete algorithm that maintains a \emph{plan
  refinement graph} ({\sc prg}). \figref{fig:prg} illustrates a simple
example with 3 high-level plans.  Every node $u$ in the {\sc prg} represents
a high-level plan $\pi_u$ and the current state of the search for a
refinement. An edge $(u,v)$ essentially represents a ``correction'' of
$\pi_u$ for a specific instantiation of the symbolic references in
$\pi_u$. We let $\pi_{u,k}$ be the plan prefix of $\pi_u$ consisting
of the first $k$ actions. Formally, each edge $e=(u,v)$ is labeled
with a tuple $\langle \sigma, k, \varphi \rangle$.  $\sigma$ is an
instantiation of references for a prefix $\pi_{u,k}$ of $\pi_u$, where
feasible motion plans have been found for all previous actions
$\pi_{u,k-1}$. $\varphi$ denotes a conjunctive formula consisting of
fluent literals that were required in the preconditions of the
$k^{th}$ action in $\pi_u$ but were not true in the state obtained
upon application of $\pi_{u,k-1}$ with the instantiation $\sigma_k$.
The plan in node $v$ (if any) retains the prefix $\pi_{u,k-1}$ and
solves the new high-level problem that incorporates the discovered
errors $\varphi$ in the $k^{th}$ state.

The overall search algorithm interleaves the search for feasible
refinements of high-level plans with the addition of new edges and plan
nodes into the {\sc prg}. This process is described using
non-deterministic choices (denoted using the prefix ``ND'') in
Alg.\,\ref{alg:complete}. Subroutine \textsc{RefineNode} selects a
reference instantiation and attempts to solve the motion planning
problems it defines. Subroutine \textsc{AddChild} selects a reference
instantiation and creates a new node that either 1) incorporates the
reason for infeasibility (provided by the subroutine
\textsc{GetError}), or 2) holds a nearly identical high-level plan,
but with a random change at a single step.  The latter can be required
in some pathological domains that have dead-ends and where changing
the instantiation of symbolic references for an action has no effect
on the action outcomes. \textsc{GetError} returns a failed
precondition for an infeasible refinement (e.g., collision-check
the current set of trajectories to detect obstructions).

Different implementations of the non-deterministic choices in
Alg.\,\ref{alg:complete} can capture various search algorithms that
account for unbounded branching factors (e.g., iterative deepening
with iterative broadening best-first search). Indeed, {\sc sfrcra-14}
can be seen as a greedy depth-first traversal of the {\sc prg}. We
show that using trained search heuristics with the {\sc prg} can
improve performance.

It is easy to see that the resulting algorithm is complete.

\begin{thm}
If there exists a high-level sequence of actions that 
a) does not revisit symbolic states when using the high-level domain
definition and b) has a motion planning feasible refinement within the scope
of symbol interpretations, then Alg.\,\ref{alg:complete} will find it,
as long as the non-deterministic policies assign non-zero weight to each choice.
\end{thm}


\begin{algorithm}[t]
\begin{small}
  \SetAlgoLined
  \DontPrintSemicolon
  \SetKwFunction{algo}{algo}\SetKwFunction{proc}{proc}
  \SetKwProg{myalg}{Algorithm}{}{}
  \SetKwProg{myproc}{Subroutine}{}{}
  \myalg{Complete {\sc tamp}}{
    \nl \For{trial in 1 ...}{
      \nl  \For{j in 1 .. trial}{
        \tcc{\footnotesize Traverse graph of plans, initially with just one plan.}
        \nl $u \leftarrow$ \textsc{NDGetNextNode}({\sc prg})\;
        \nl $\pi \leftarrow$ \textsc{GetHLPlan}($u$)\;
        \nl mode $\leftarrow$ \textsc{NDChoice}\{refine, add child\}\;
        \nl \eIf{mode == refine}{ \textsc{RefineNode}($\pi$, $j$)}{
        \textsc{AddChild}($\pi$, $j$) }}}
  \;
  \setcounter{AlgoLine}{0}
  \myproc{\textsc{RefineNode}($\pi$, $j$)}{
    \nl $\sigma \leftarrow$ \textsc{NDGetInstantiation}($\pi$, $j$)\;
    \tcc{\footnotesize resourceLimit($j$) is monotonically increasing in $j$.}
    \nl  MP, FailedAction, FailedPred $\leftarrow$ \textsc{GetMotionPlan}($\sigma$, $\pi$, resourceLimit($j$))\;
    \nl \If{MP $\ne$ NULL}{
      \nl return success\;
    }
    }\;
    \;
  \setcounter{AlgoLine}{0}
  \myproc{\textsc{AddChild}($\pi$, $j$)}{
    \nl $\sigma \leftarrow$ \textsc{NDGetInstantiation}($\pi$, $j$) \;
    \nl StepNum, FailedPrecon $\leftarrow$ \textsc{GetError}($\sigma$, $\pi$)\;
    \nl mode $\leftarrow$ \textsc{NDChoice}\{error, random\}\;
    \nl \eIf{mode == error} {
      \nl NewState $\leftarrow$ \textsc{Patch}(\textsc{GetStateAt}(StepNum,
      $\pi$), FailedPrecon)\;
    }{
      \nl $\pi\leftarrow$ $\pi$, with an action
      before StepNum replaced $~~~~~~~~~~~$by a random applicable action\;
      \nl NewState $\leftarrow$ \textsc{GetStateAt}(StepNum, $\pi$)\;
    }
    \nl $\pi'\leftarrow$ \textsc{GetClassicalPlan}(NewState)\;
    \nl \textsc{AddNodeToPRG}($\sigma$, StepNum, $\pi'$)\;
  }}
\end{small}
\caption{Complete algorithm for {\sc tamp}.}
\label{alg:complete}
\end{algorithm}


The proof follows easily because if there is a solution, then
the non-deterministic calls can be selected appropriately to find
it. In the next section, we show a specific implementation of \textsc{RefineNode}
based on randomization. Afterward, we show how to train
heuristics that guide the search processes, replacing the non-deterministic choices.



