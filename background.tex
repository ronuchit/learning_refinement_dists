\section{Background}
\subsection{Task and Motion Planning}

A motion planning problem is defined by a configuration space for a
robot and all movable objects in its environment, along with initial
and final configurations. The solution to a motion planning problem is
a collision-free trajectory for the robot that connects these
configurations.  In task and motion planning, we add more abstract
concepts to this formulation.

% For example, we can use the action schema \emph{grasp(Object o,
%   Manipulator p, GraspingPose g, Trajectory m)} to abstractly
% represent grasping an object, \emph{o}.  In order to apply this action,
% the agent must select values for each of these parameters (e.g.,
% \emph{grasp(can$_1$, left, g$_1$, m$_1$)}). These \emph{instantiated}
% actions change the value of specific fluent instantiations (also
% called fluent \emph{literals}), such as \emph{in-gripper(can$_1$, gripper$_l$)}
% and \emph{empty(gripper$_l$)}.

\begin{defn}
We define a task and motion planning ({\sc tamp}) problem as a tuple
$\langle \T, \Obj, \F, \I, \G, \U \rangle$:
\begin{tightlist}
\item $\T$: a set of object \emph{types} (e.g., movable objects,
  trajectories, poses, and locations).

\item $\Obj$: a set of \emph{objects} (e.g., cans, trajectories, and
  poses). $\Obj$ defines the configuration space of all movable
  objects, including the robot.

\item $\F$: a set of \emph{fluents}, which define relationships among
  objects and are Boolean functions defined over the configuration
  space.

\item $\I$: a conjunction of fluents that defines the initial state.

\item $\G$: a conjunction of fluents that defines the goal state.

\item $\U$: a set of \emph{high-level actions} (e.g., grasp, move, and
  putdown), parameterized by objects and defined by
  \emph{preconditions}, a set of fluents that describe when an action
  can be taken; and \emph{effects}, a set of fluents that hold true
  after the action is performed.
\end{tightlist}
\end{defn}

An instantiated action is said to be \emph{feasible} in a state if and
only if its preconditions hold in that state. A solution to a {\sc
  tamp} problem is a sequence of instantiated actions $a_{0}, a_{1},
..., a_{n} \in \U$ such that every action is feasible when applied
successively starting with $\I$, and the final state reached satisfies the
goal condition $\G$.

% As a simple example, we consider a pick domain where the robot is capable of
% performing only two actions: moving to a location and grasping an object. We can
% then specify the planning problem under our representation for the goal of holding
% a particular object $obj_{1}$ in the robot's right gripper.

% \begin{tightlist}
% \item[$\Obj$:] Objects in the environment $obj_{0}, ..., obj_{n}$, object initial poses,
% robot initial pose, and left gripper and right gripper grasping poses
% for each object. $\Obj$ is a set of symbolic references, represented at the high
% level only as strings with no geometric interpretation.

% \item[$\F$:] \emph{Obstructs}$(traj, obj_{0}, obj_{1})$, \emph{InGripper}$(obj, gripper)$,\\\emph{Empty}$(gripper)$,
% \emph{IsGraspPose}$(pose, obj, gripper)$,\\\emph{At}$(obj, pose)$, \emph{RobotAt}$(pose)$,\\
% \emph{IsValidTraj}$(traj, pose_{0}, pose_{1})$. Here, the \emph{IsValidTraj} predicate checks that $traj$
% joins $pose_{0}$ with $pose_{1}$, and that it is feasible to execute and
% collision-free.

% \item[$\I$:] \emph{IsGraspPose} between all grasping poses and their corresponding object,
% \emph{RobotAt} the robot's initial pose, \emph{At} for all object initial poses, and
% \emph{Empty} for both grippers.

% \item[$\G$:] \emph{InGripper}$(obj_{1}, right\_gripper)$.

% \item[$\U$:]
% \begin{tightlist}
% \item[1)] \emph{MoveTo}
%   \begin{tightlist}
%   \item[params:]$pose_{0}, pose_{1}, traj$
%   \item[preconds:]\emph{RobotAt}$(pose_{0}) \wedge$\\ \emph{IsValidTraj}$(traj, pose_{0}, pose_{1})$
%   \item[effects:]\emph{RobotAt}$(pose_{1}) \wedge \lnot$\emph{RobotAt}$(pose_{0})$
%   \end{tightlist}
% \item[2)] \emph{Grasp}
%   \begin{tightlist}
%   \item[params:]$obj, obj\_pose, robot\_pose,\\grasp\_pose, gripper, traj$
%   \item[preconds:]\emph{RobotAt}$(robot\_pose) \wedge$\\ \emph{Empty}$(gripper) \wedge$
% \emph{At}$(obj, obj\_pose) \wedge$\\ \emph{IsGraspPose}$(grasp\_pose, obj, gripper) \wedge$\\ \emph{IsValidTraj}$(traj, robot\_pose, grasp\_pose)$
%   \item[effects:]\emph{RobotAt}$(grasp\_pose) \wedge$ $\lnot$\emph{RobotAt}$(robot\_pose) \wedge \lnot$\emph{Empty}$(gripper) \wedge\\
% \lnot$\emph{At}$(obj, obj\_pose) \wedge$ \emph{InGripper}$(obj, gripper) \wedge \forall\ traj', o: \lnot$\emph{Obstructs}$(traj', obj, o)$
%   \end{tightlist}
% \end{tightlist}
% \end{tightlist}

% For the \emph{Grasp} action, the fact that the path to $obj$ must be collision-free is
% implicitly checked within the \emph{IsValidTraj} precondition. An important aspect of
% this formulation is the assumption that grasping an object causes it to no longer
% obstruct any other object in the environment. If no other object obstructs $obj_{1}$,
% a possible high-level plan with a feasible refinement for achieving $\G$ is
% \begin{tightlist}
% \item[1.] \emph{MoveTo}$(robot\_init\_pose, rgripper\_bp\_obj_{1})$
% \item[2.] \emph{Grasp}$(obj_{1}, obj_{1}\_init\_pose, rgripper\_bp\_obj_{1},\\rgripper\_gp\_obj_{1}, rgripper)$
% \end{tightlist}

% The $bp$ parameters refer to robot base poses in preparation for grasping an object.
% The trajectory parameters are not included here because they are based on the interface layer's
% sampling of the base pose and grasping pose. If $obj_{1}$ is
% obstructed by, say, $obj_{3}$ in the environment, a possible high-level plan would involve
% grasping first $obj_{3}$ then $obj_{1}$.




% For example, consider the previous pick domain specification where $obj_{3}$ does obstruct $obj_{1}$.
% Initially, the task planner returns a high-level plan to immediately grasp $obj_{1}$, because
% it is not yet aware of the obstruction. The interface layer then samples grasp poses
% for $obj_{1}$, but motion planning for each sample fails due to the obstruction. Eventually,
% this error is propagated back to the task planner. The new high-level plan correctly
% grasps $obj_{3}$ before $obj_{1}$, and refining this plan succeeds.

% For example, consider a pick domain with two objects: a target $o_{t}$ to be grasped and an obstruction.
% Initially, the task planner returns a high-level plan to immediately grasp $o_{t}$, because
% it is not yet aware of the obstruction. The interface layer then samples grasp poses
% for $o_{t}$, but motion planning for each sample fails due to the obstruction. Eventually,
% this error information about the obstruction is converted into a symbolic representation
% and propagated back to the task planner. The new high-level plan correctly grasps the obstruction
% before grasping $o_{t}$, and refining this plan succeeds.

\subsection{Markov Decision Processes}
Markov decision processes ({\sc mdp}s) formalize the interaction
between an agent and an environment. At each step of an {\sc mdp}, the
agent knows the current state and selects an action. This causes the
state to change according to a known transition distribution.
\begin{defn}
We define a finite-horizon {\sc mdp} as a tuple $\langle \St, \A, T,
R, H, \Prob \rangle$, where
\begin{tightlist}
\item $\St$ is the state space.
\item $\A$ is the action space.
\item $T(s, a, s') = Pr(s' \mid s, a)$ for $s, s' \in \St, a \in \A$ is the transition distribution.
\item $R(s, a, s')$ for $s, s' \in \St, a \in \A$ is the reward function.
\item $H$ is the horizon, or total number of timesteps.
\item $\Prob$ is the initial state distribution.
\end{tightlist}
\end{defn}
A solution to an {\sc mdp} is a policy, $\pi: \St \times \mathbb{Z} \rightarrow \A$,
that maps the state and timestep to an action.  The
value function under $\pi$ is a function of the timestep $k$ and state
$s$:
$$V_{\pi}^{k}(s) = \mathbb{E}\left[\sum_{t=k}^{H}R(s_{t}) \mid \pi, s_{k} = s\right].$$

In reinforcement learning ({\sc rl}), an agent must determine
$\pi^{*}$ through interaction with its environment (i.e., without
explicit access to $\St$ or $T$). At each timestep, the agent knows
the state and what actions are available, but initially does not know
how taking actions will affect the state. There is a large body of
research on {\sc rl}. Standard techniques include value function
approximation, which uses methods such as temporal difference
learning, and direct policy estimation, which encompasses
gradient-based and gradient-free methods~\cite{suttonbarto}.

In \emph{inverse reinforcement learning} ({\sc
  irl})~\cite{ng2000algorithms}, an agent attempts to recover $R$ from
a description of the {\sc mdp} and execution traces of optimal
behavior. This is useful in scenarios where an expert demonstrator can
help guide learning.  Some standard techniques include maximum-margin
{\sc irl}~\cite{abbeel2004apprenticeship} and maximum-entropy {\sc
  irl}~\cite{maxentirl}.
