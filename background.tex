\section{Background}
We provide relevant technical background and introduce notation
used throughout the paper. 

\subsection{Task and Motion Planning}

A motion planning problem is a defined as a tuple $\tuple{\X, f, p_0,
  p_t}$, where $\X$ is the space of possible configurations or poses
of a robot, $f$ is a Boolean function that determines whether or not a
pose is in collision, and $p_0, p_t\in C$ are the initial and final
poses. The solution to a motion planning problem is a collision-free trajectory that
connects $p_0$ and $p_t$. To allow for object
manipulation, we let $\X$ include poses for each movable object in the
environment. 

In task and motion planning, we add more abstract concepts to this
formulation, including \emph{fluents} (logical properties that hold either
true or false and may
vary across configurations) and \emph{actions} (operations that the
agent may choose to execute, resulting in changes to the configuration
and the set of fluents that are true). Each action may require motion
planning prior to execution.  The overall problem is to plan the
sequence of actions that the agent can execute to achieve a
desired goal condition expressed in terms of fluents.

For example, we can use the action schema \emph{grasp(Object o,
  Manipulator p, GraspingPose g, Trajectory m)} to abstractly
represent grasping an object, \emph{o}.  In order to apply this action,
the agent must select values for each of these parameters (e.g.,
\emph{grasp(can$_1$, left, g$_1$, m$_1$)}). These \emph{instantiated}
actions change the value of specific fluent instantiations (also
called fluent \emph{literals}), such as \emph{in-gripper(can$_1$, gripper$_l$)}
and \emph{empty(gripper$_l$)}.

\begin{defn}
Formally, we define the task and motion planning (TAMP) problem as a tuple $\langle \Obj,
 \T, \F, \I, \G, \U \rangle$:
\begin{tightlist}
\item $\Obj$ is a set of objects denoting elements such as cans,
  trajectories, and poses. Note that $\Obj$ includes the
  configuration space of all movable objects, including the robot.
\item $\T$ is a set of object \emph{types}, such as movable objects, motion plans, poses, and locations.
\item $\F$ is a set of \emph{fluents}, which define relationships
  among objects and are Boolean functions defined over the configuration space.
\item $\I$ is the set of fluent literals that hold true in the initial state.
\item $\G$ is the set of fluent literals defining the goal condition.
\item $\U$ is a set of \emph{high-level actions} that are
  parameterized using objects and defined by \emph{preconditions}, a set
of fluent literals that must hold true in the current state to be able to perform the action;
and \emph{effects}, a set of fluent literals that hold true after the
action is performed. 
\end{tightlist}
\end{defn}

An instantiated action is said to be \emph{feasible} in a state if and only if its
preconditions hold in that state. Implicitly, the trajectories corresponding to actions
must be collision-free.

A solution to a TAMP problem is a sequence of instantiated
actions $a_{0}, a_{1}, ..., a_{n} \in \U$ such that every action is
feasible when it is applied on states successively starting with $\I$, and the
state achieved at the end of the execution sequence satisfies the
goal condition $\G$.

% As a simple example, we consider a pick domain where the robot is capable of
% performing only two actions: moving to a location and grasping an object. We can
% then specify the planning problem under our representation for the goal of holding
% a particular object $obj_{1}$ in the robot's right gripper.

% \begin{tightlist}
% \item[$\Obj$:] Objects in the environment $obj_{0}, ..., obj_{n}$, object initial poses,
% robot initial pose, and left gripper and right gripper grasping poses
% for each object. $\Obj$ is a set of symbolic references, represented at the high
% level only as strings with no geometric interpretation.

% \item[$\F$:] \emph{Obstructs}$(traj, obj_{0}, obj_{1})$, \emph{InGripper}$(obj, gripper)$,\\\emph{Empty}$(gripper)$,
% \emph{IsGraspPose}$(pose, obj, gripper)$,\\\emph{At}$(obj, pose)$, \emph{RobotAt}$(pose)$,\\
% \emph{IsValidTraj}$(traj, pose_{0}, pose_{1})$. Here, the \emph{IsValidTraj} predicate checks that $traj$
% joins $pose_{0}$ with $pose_{1}$, and that it is feasible to execute and
% collision-free.

% \item[$\I$:] \emph{IsGraspPose} between all grasping poses and their corresponding object,
% \emph{RobotAt} the robot's initial pose, \emph{At} for all object initial poses, and
% \emph{Empty} for both grippers.

% \item[$\G$:] \emph{InGripper}$(obj_{1}, right\_gripper)$.

% \item[$\U$:]
% \begin{tightlist}
% \item[1)] \emph{MoveTo}
%   \begin{tightlist}
%   \item[params:]$pose_{0}, pose_{1}, traj$
%   \item[preconds:]\emph{RobotAt}$(pose_{0}) \wedge$\\ \emph{IsValidTraj}$(traj, pose_{0}, pose_{1})$
%   \item[effects:]\emph{RobotAt}$(pose_{1}) \wedge \lnot$\emph{RobotAt}$(pose_{0})$
%   \end{tightlist}
% \item[2)] \emph{Grasp}
%   \begin{tightlist}
%   \item[params:]$obj, obj\_pose, robot\_pose,\\grasp\_pose, gripper, traj$
%   \item[preconds:]\emph{RobotAt}$(robot\_pose) \wedge$\\ \emph{Empty}$(gripper) \wedge$
% \emph{At}$(obj, obj\_pose) \wedge$\\ \emph{IsGraspPose}$(grasp\_pose, obj, gripper) \wedge$\\ \emph{IsValidTraj}$(traj, robot\_pose, grasp\_pose)$
%   \item[effects:]\emph{RobotAt}$(grasp\_pose) \wedge$ $\lnot$\emph{RobotAt}$(robot\_pose) \wedge \lnot$\emph{Empty}$(gripper) \wedge\\
% \lnot$\emph{At}$(obj, obj\_pose) \wedge$ \emph{InGripper}$(obj, gripper) \wedge \forall\ traj', o: \lnot$\emph{Obstructs}$(traj', obj, o)$
%   \end{tightlist}
% \end{tightlist}
% \end{tightlist}

% For the \emph{Grasp} action, the fact that the path to $obj$ must be collision-free is
% implicitly checked within the \emph{IsValidTraj} precondition. An important aspect of
% this formulation is the assumption that grasping an object causes it to no longer
% obstruct any other object in the environment. If no other object obstructs $obj_{1}$,
% a possible high-level plan with a feasible refinement for achieving $\G$ is
% \begin{tightlist}
% \item[1.] \emph{MoveTo}$(robot\_init\_pose, rgripper\_bp\_obj_{1})$
% \item[2.] \emph{Grasp}$(obj_{1}, obj_{1}\_init\_pose, rgripper\_bp\_obj_{1},\\rgripper\_gp\_obj_{1}, rgripper)$
% \end{tightlist}

% The $bp$ parameters refer to robot base poses in preparation for grasping an object.
% The trajectory parameters are not included here because they are based on the interface layer's
% sampling of the base pose and grasping pose. If $obj_{1}$ is
% obstructed by, say, $obj_{3}$ in the environment, a possible high-level plan would involve
% grasping first $obj_{3}$ then $obj_{1}$.




% For example, consider the previous pick domain specification where $obj_{3}$ does obstruct $obj_{1}$.
% Initially, the task planner returns a high-level plan to immediately grasp $obj_{1}$, because
% it is not yet aware of the obstruction. The interface layer then samples grasp poses
% for $obj_{1}$, but motion planning for each sample fails due to the obstruction. Eventually,
% this error is propagated back to the task planner. The new high-level plan correctly
% grasps $obj_{3}$ before $obj_{1}$, and refining this plan succeeds.

% For example, consider a pick domain with two objects: a target $o_{t}$ to be grasped and an obstruction.
% Initially, the task planner returns a high-level plan to immediately grasp $o_{t}$, because
% it is not yet aware of the obstruction. The interface layer then samples grasp poses
% for $o_{t}$, but motion planning for each sample fails due to the obstruction. Eventually,
% this error information about the obstruction is converted into a symbolic representation
% and propagated back to the task planner. The new high-level plan correctly grasps the obstruction
% before grasping $o_{t}$, and refining this plan succeeds.

\subsection{Markov Decision Processes and Reinforcement Learning}
Markov decision processes (MDPs) provide a way to formalize
interactions between agents and environments. At each step of an MDP,
the agent knows its current state and selects an action. This causes the
state to change according to a known transition distribution.
\begin{defn}
Formally, we define an MDP as a tuple $\langle \St, \A, T, R, \gamma, \Prob \rangle$, where
\begin{tightlist}
\item $\St$ is the state space.
\item $\A$ is the action space.
\item $T(s, a, s') = Pr(s' | s, a)$ for $s, s' \in \St, a \in \A$ is the transition distribution.
\item $R(s, a, s')$ for $s, s' \in \St, a \in \A$ is the reward function.
\item $\gamma \in [0, 1]$ is the discount factor.
\item $\Prob$ is the initial state distribution.
\end{tightlist}
\end{defn}
A solution to an MDP is a policy, $\pi: \St \rightarrow \A$, that maps states to
actions. The value, $V_{\pi}(s)$, of a state under $\pi$ is the sum of expected
discounted future rewards generated from starting in state $s$ and selecting actions according
to $\pi$:
$$V_{\pi}(s) = \mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t}) | \pi, s_{0} = s].$$
The optimal policy, $\pi^{*}$, maximizes this value for all states.

In reinforcement learning (RL), an agent must determine $\pi^{*}$
through interaction with its environment (i.e. without explicit access
to $\St$ or $T$). At each timestep, the agent knows the state and what
actions are available, but initially does not know how taking actions will
affect the state. There is a large body of research on RL, and
standard techniques include value function approximation, which uses methods such as temporal difference
(TD) learning, and direct policy estimation, which encompasses both gradient-based
and gradient-free methods~\cite{suttonbarto}.

\subsection{Reinforcement Learning for Planning}
Our problem formulation is motivated by Zhang and Dietterich's application of RL to job
shop scheduling~\cite{JobShopSched}. Job shop scheduling is a combinatorial optimization problem where the goal is to find
a minimum-duration schedule of a set of jobs with temporal and resource constraints. An empirically
successful approach to this problem relies on a randomized local search that proposes changes to an
existing suboptimal schedule. The authors formulate this as an MDP and use $TD(\lambda)$~\cite{suttonbarto} with function
approximation to learn a value function for it. Their approach outperforms the previous state of the art for this task and
scales better to larger scheduling problems.

Zucker et al.~\cite{workspacebias} use RL to bias the distribution of a rapidly exploring random tree (RRT)
for motion planning. Their approach uses features of a discretization of the workspace to train
a non-uniform configuration space sampler using policy gradient algorithms.
In our work, we adapt their gradient updates to the TAMP framework (Section V-C).
