\section{Background}
We provide relevant technical background and introduce notation
used throughout the paper.

\subsection{Reinforcement Learning}
A reinforcement learning problem is typically formulated as
a Markov decision process (MDP). An MDP is defined by the following:

\begin{tightlist}
\item A state space $S$.
\item An action space $A$.
\item A transition function $P(s' | s, a)$ for $s, s' \in S, a \in A$.
\item A reward function $R(s, a, s')$ for $s, s' \in S, a \in A$.
\item A discount factor $\gamma \in [0, 1]$.
\end{tightlist}
The goal of a reinforcement learning problem is to train a policy $\pi(s): S \rightarrow A$
that selects which action $a \in A$ to execute given a current state $s \in S$, in
order to maximize the sum of expected discounted future rewards.

\subsection{Reinforcement Learning for Planning}
Zhang and Dietterich~\cite{JobShopSched} present a technique for formulating the job shop scheduling
problem using the reinforcement learning framework. The goal of job shop scheduling is to find
a minimum-duration schedule of a set of jobs with temporal and resource constraints. They define the following:
\begin{tightlist}
\item $S$ is all temporal configurations and resource allocations of the jobs.
\item $A$ is changing either the time or resource allocation of a single job.
\item $R$ is a measure of schedule goodness based on its length relative to the difficulty of the scheduling problem.
\end{tightlist}
They then use the temporal difference learning algorithm $TD(\lambda)$~\cite{suttonbarto}
to learn a heuristic state evaluation function, used in a one-step lookahead search
to produce good schedules. Their approach outperforms the previous state of the art for this task;
it also scales better to larger scheduling problems after having been trained only on smaller ones.
This application of reinforcement learning to planning has been built on extensively
in the literature, applied to domains such as retailer inventory
management~\cite{van1997neuro}, single-agent manufacturing systems~\cite{Wang200573}, and multi-robot
task allocation~\cite{dahl2009multi}.

Zucker et al.~\cite{workspacebias} use reinforcement learning to find good sampling distributions
for motion planning. Their approach uses features of a discretization of the workspace to train
a non-uniform configuration space sampler. They use stochastic policy gradient updates to learn a good weight
vector $\theta$ for these features. Toward this end, they define the distribution
\begin{equation}
q(\theta, x) \propto exp(\theta^{T} f(x))
\end{equation}
They also define the expected reward under $q$ of an episode $\xi$ with length $\epsilon$:
\begin{equation}
\eta(\theta) = E_{q}[R(\xi)]
\end{equation}
and provide an approximation for its gradient:
\begin{equation}
\nabla \eta(\theta) \approx \frac{R(\xi)}{\epsilon} \sum_{i=1}^{\epsilon}(f(x_{i}) - E_{q}[f])
\end{equation}
where $E_{q}[f]$ is the feature vector expectation under $q$. The weight vector update is then:
\begin{equation}
\theta \leftarrow \theta + \alpha \nabla \eta(\theta)
\end{equation}
for appropriate step size $\alpha$.

\subsection{Task and Motion Planning}
We formulate a task and motion planning problem as follows. A fully observed task planning
problem is a tuple $(\Obj, \F, \I, \G, \C)$:

$\Obj$ is a set of symbolic references to \emph{objects} in the environment,
such as cylinders, cylinder locations, and grasping poses. These are treated as strings.

$\F$ is a set of fluents, which define relationships among objects and hold either
true or false in the state.

$\I$ is the set of fluents that hold true in the initial state. All others are assumed false.

$\G$ is the set of fluents defining the goal state.

$\C$ is a set of parametrized \emph{high level actions}, defined by \emph{preconditions}, a set
of fluents characterizing what must hold true in the current state for the action
to be validly peformed, and \emph{effects}, a set of fluents that hold true after
the action is performed.

A solution to a task planning problem is a valid sequence of actions
$a_{0}, a_{1}, ..., a_{n} \in \C$ which, when applied to the state of fluents
successively starting with $\I$, results in a state in which all fluents of
$\G$ hold true. We refer to this sequence of actions as a \emph{symbolic}, or
\emph{high level}, plan: the task planner maintains no knowledge about the geometric meaning
behind any of the symbols.

% As a simple example, we consider a pick domain where the robot is capable of
% performing only two actions: moving to a location and grasping an object. We can
% then specify the planning problem under our representation for the goal of holding
% a particular object $obj_{1}$ in the robot's right gripper.

% \begin{tightlist}
% \item[$\Obj$:] Objects in the environment $obj_{0}, ..., obj_{n}$, object initial poses,
% robot initial pose, and left gripper and right gripper grasping poses
% for each object. $\Obj$ is a set of symbolic references, represented at the high
% level only as strings with no geometric interpretation.

% \item[$\F$:] \emph{Obstructs}$(traj, obj_{0}, obj_{1})$, \emph{InGripper}$(obj, gripper)$,\\\emph{Empty}$(gripper)$,
% \emph{IsGraspPose}$(pose, obj, gripper)$,\\\emph{At}$(obj, pose)$, \emph{RobotAt}$(pose)$,\\
% \emph{IsValidTraj}$(traj, pose_{0}, pose_{1})$. Here, the \emph{IsValidTraj} predicate checks that $traj$
% joins $pose_{0}$ with $pose_{1}$, and that it is feasible to execute and
% collision-free.

% \item[$\I$:] \emph{IsGraspPose} between all grasping poses and their corresponding object,
% \emph{RobotAt} the robot's initial pose, \emph{At} for all object initial poses, and
% \emph{Empty} for both grippers.

% \item[$\G$:] \emph{InGripper}$(obj_{1}, right\_gripper)$.

% \item[$\C$:]
% \begin{tightlist}
% \item[1)] \emph{MoveTo}
%   \begin{tightlist}
%   \item[params:]$pose_{0}, pose_{1}, traj$
%   \item[preconds:]\emph{RobotAt}$(pose_{0}) \wedge$\\ \emph{IsValidTraj}$(traj, pose_{0}, pose_{1})$
%   \item[effects:]\emph{RobotAt}$(pose_{1}) \wedge \lnot$\emph{RobotAt}$(pose_{0})$
%   \end{tightlist}
% \item[2)] \emph{Grasp}
%   \begin{tightlist}
%   \item[params:]$obj, obj\_pose, robot\_pose,\\grasp\_pose, gripper, traj$
%   \item[preconds:]\emph{RobotAt}$(robot\_pose) \wedge$\\ \emph{Empty}$(gripper) \wedge$
% \emph{At}$(obj, obj\_pose) \wedge$\\ \emph{IsGraspPose}$(grasp\_pose, obj, gripper) \wedge$\\ \emph{IsValidTraj}$(traj, robot\_pose, grasp\_pose)$
%   \item[effects:]\emph{RobotAt}$(grasp\_pose) \wedge$ $\lnot$\emph{RobotAt}$(robot\_pose) \wedge \lnot$\emph{Empty}$(gripper) \wedge\\
% \lnot$\emph{At}$(obj, obj\_pose) \wedge$ \emph{InGripper}$(obj, gripper) \wedge \forall\ traj', o: \lnot$\emph{Obstructs}$(traj', obj, o)$
%   \end{tightlist}
% \end{tightlist}
% \end{tightlist}

% For the \emph{Grasp} action, the fact that the path to $obj$ must be collision-free is
% implicitly checked within the \emph{IsValidTraj} precondition. An important aspect of
% this formulation is the assumption that grasping an object causes it to no longer
% obstruct any other object in the environment. If no other object obstructs $obj_{1}$,
% a possible high level plan with a feasible refinement for achieving $\G$ is
% \begin{tightlist}
% \item[1.] \emph{MoveTo}$(robot\_init\_pose, rgripper\_bp\_obj_{1})$
% \item[2.] \emph{Grasp}$(obj_{1}, obj_{1}\_init\_pose, rgripper\_bp\_obj_{1},\\rgripper\_gp\_obj_{1}, rgripper)$
% \end{tightlist}

% The $bp$ parameters refer to robot base poses in preparation for grasping an object.
% The trajectory parameters are not included here because they are based on the interface layer's
% sampling of the base pose and grasping pose. If $obj_{1}$ is
% obstructed by, say, $obj_{3}$ in the environment, a possible high level plan would involve
% grasping first $obj_{3}$ then $obj_{1}$.

Any continuous variables of a high level plan, such as grasping poses for
objects, are abstracted into symbols representing their nature. Task
planners are, thus, unable to maintain a geometric interpretation for these symbols, so we use
hierarchical task and motion planning to provide this interpretation. We adopt the approach of
Srivastava et al.~\cite{srivastava2014combined}. They describe an \emph{interface layer} responsible
for assigning continuous values to the high level plan variables, discovering geometric facts about
the environment, and sending these facts back to the task planner to be added into the fluent state.
This process of assigning such continuous values to the abstract plan variables is known as \emph{plan refinement}.
The authors introduce an algorithm for plan refinement called \textsc{TryRefine}, an exhaustive
backtracking routine over feasible high level plan parameter values.
% For example, consider the previous pick domain specification where $obj_{3}$ does obstruct $obj_{1}$.
% Initially, the task planner returns a high level plan to immediately grasp $obj_{1}$, because
% it is not yet aware of the obstruction. The interface layer then samples grasp poses
% for $obj_{1}$, but motion planning for each sample fails due to the obstruction. Eventually,
% this error is propagated back to the task planner. The new high level plan correctly
% grasps $obj_{3}$ before $obj_{1}$, and refining this plan succeeds.

% For example, consider a pick domain with two objects: a target $o_{t}$ to be grasped and an obstruction.
% Initially, the task planner returns a high level plan to immediately grasp $o_{t}$, because
% it is not yet aware of the obstruction. The interface layer then samples grasp poses
% for $o_{t}$, but motion planning for each sample fails due to the obstruction. Eventually,
% this error information about the obstruction is converted into a symbolic representation
% and propagated back to the task planner. The new high level plan correctly grasps the obstruction
% before grasping $o_{t}$, and refining this plan succeeds.