\section{Background}
We provide relevant technical background and introduce notation
used throughout the paper.

\subsection{Markov Decision Processes and Reinforcement Learning}
Markov decision processes (MDPs) are the standard AI approach for formulating
interactions between agents and environments. At each step of an MDP,
the agent knows its current state and selects an action, causing the
state to transition according to a known transition distribution. Formally,
an MDP is a tuple $\langle \St, \A, T, R, \gamma, \Prob \rangle$:
\begin{tightlist}
\item $\St$ is the state space.
\item $\A$ is the action space.
\item $T(s, a, s') = Pr(s' | s, a)$ for $s, s' \in \St, a \in \A$ is the transition distribution.
\item $R(s, a, s')$ for $s, s' \in \St, a \in \A$ is the reward function.
\item $\gamma \in [0, 1]$ is the discount factor.
\item $\Prob$ is the initial state distribution.
\end{tightlist}
A solution to an MDP is a policy $\pi: \St \rightarrow \A$ that maps states to
actions. The value $V_{\pi}(s)$ of a state under $\pi$ is the sum of expected
discounted future rewards from starting in state $s$ and selecting actions according
to $\pi$:
$$V_{\pi}(s) = \mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t}) | \pi, s_{0} = s].$$
The optimal policy $\pi^{*}$ maximizes this value for all states.

In reinforcement learning (RL), an agent must determine $\pi^{*}$
through interaction with its environment (i.e. without explicit access
to $\St$ or $T$). At each timestep, the agent knows the state and what
actions are available, but initially does not know how taking actions will
affect the state. There is a large body of research on RL, and
standard techniques include value function approximation, which uses methods such as temporal difference
(TD) learning, and direct policy estimation, which encompasses both gradient-based
and gradient-free methods~\cite{suttonbarto}.

\subsection{Reinforcement Learning for Planning}
Our problem formulation is motivated by Zhang and Dietterich's application of RL to job
shop scheduling~\cite{JobShopSched}. Job shop scheduling is a combinatorial optimization problem where the goal is to find
a minimum-duration schedule of a set of jobs with temporal and resource constraints. An empirically
successful approach to this problem relies on a randomized local search that proposes changes to an
existing suboptimal schedule. The authors formulate this as an MDP and use $TD(\lambda)$~\cite{suttonbarto} with function
approximation to learn a value function for it. Their approach outperforms the previous state of the art for this task and
scales better to larger scheduling problems.

Zucker et al.~\cite{workspacebias} use RL to bias the distribution of a rapidly exploring random tree (RRT)
for motion planning. Their approach uses features of a discretization of the workspace to train
a non-uniform configuration space sampler using policy gradient algorithms.
In our work, we adopt their gradient updates for the TAMP framework (Section V-C).

\subsection{Task and Motion Planning}
We define task and motion planning (TAMP) as a tuple $\langle \Obj, \T, \F, \I, \G, \C \rangle$:
\begin{tightlist}
\item $\Obj$ is a set of symbolic references to \emph{objects} in the environment,
such as cylinders and grasping poses.
\item $\T$ is the set of object \emph{types}, such as robot manipulators, cylinders, poses, and locations.
\item $\F$ is a set of \emph{fluents}, which define relationships among objects and hold either
true or false in the state.
\item $\I$ is the set of fluents that hold true in the initial state.
\item $\G$ is the set of fluents defining the goal state.
\item $\C$ is a set of parametrized \emph{high level actions}, defined by \emph{preconditions}, a set
of fluents that must hold true in the current state to be able to perform the action,
and \emph{effects}, a set of fluents that hold true after the action is performed.
\end{tightlist}
A solution to a task planning problem is a valid sequence of actions
$a_{0}, a_{1}, ..., a_{n} \in \C$ which, when applied to the state of fluents
successively starting with $\I$, results in a state in which all fluents of
$\G$ hold true.

% As a simple example, we consider a pick domain where the robot is capable of
% performing only two actions: moving to a location and grasping an object. We can
% then specify the planning problem under our representation for the goal of holding
% a particular object $obj_{1}$ in the robot's right gripper.

% \begin{tightlist}
% \item[$\Obj$:] Objects in the environment $obj_{0}, ..., obj_{n}$, object initial poses,
% robot initial pose, and left gripper and right gripper grasping poses
% for each object. $\Obj$ is a set of symbolic references, represented at the high
% level only as strings with no geometric interpretation.

% \item[$\F$:] \emph{Obstructs}$(traj, obj_{0}, obj_{1})$, \emph{InGripper}$(obj, gripper)$,\\\emph{Empty}$(gripper)$,
% \emph{IsGraspPose}$(pose, obj, gripper)$,\\\emph{At}$(obj, pose)$, \emph{RobotAt}$(pose)$,\\
% \emph{IsValidTraj}$(traj, pose_{0}, pose_{1})$. Here, the \emph{IsValidTraj} predicate checks that $traj$
% joins $pose_{0}$ with $pose_{1}$, and that it is feasible to execute and
% collision-free.

% \item[$\I$:] \emph{IsGraspPose} between all grasping poses and their corresponding object,
% \emph{RobotAt} the robot's initial pose, \emph{At} for all object initial poses, and
% \emph{Empty} for both grippers.

% \item[$\G$:] \emph{InGripper}$(obj_{1}, right\_gripper)$.

% \item[$\C$:]
% \begin{tightlist}
% \item[1)] \emph{MoveTo}
%   \begin{tightlist}
%   \item[params:]$pose_{0}, pose_{1}, traj$
%   \item[preconds:]\emph{RobotAt}$(pose_{0}) \wedge$\\ \emph{IsValidTraj}$(traj, pose_{0}, pose_{1})$
%   \item[effects:]\emph{RobotAt}$(pose_{1}) \wedge \lnot$\emph{RobotAt}$(pose_{0})$
%   \end{tightlist}
% \item[2)] \emph{Grasp}
%   \begin{tightlist}
%   \item[params:]$obj, obj\_pose, robot\_pose,\\grasp\_pose, gripper, traj$
%   \item[preconds:]\emph{RobotAt}$(robot\_pose) \wedge$\\ \emph{Empty}$(gripper) \wedge$
% \emph{At}$(obj, obj\_pose) \wedge$\\ \emph{IsGraspPose}$(grasp\_pose, obj, gripper) \wedge$\\ \emph{IsValidTraj}$(traj, robot\_pose, grasp\_pose)$
%   \item[effects:]\emph{RobotAt}$(grasp\_pose) \wedge$ $\lnot$\emph{RobotAt}$(robot\_pose) \wedge \lnot$\emph{Empty}$(gripper) \wedge\\
% \lnot$\emph{At}$(obj, obj\_pose) \wedge$ \emph{InGripper}$(obj, gripper) \wedge \forall\ traj', o: \lnot$\emph{Obstructs}$(traj', obj, o)$
%   \end{tightlist}
% \end{tightlist}
% \end{tightlist}

% For the \emph{Grasp} action, the fact that the path to $obj$ must be collision-free is
% implicitly checked within the \emph{IsValidTraj} precondition. An important aspect of
% this formulation is the assumption that grasping an object causes it to no longer
% obstruct any other object in the environment. If no other object obstructs $obj_{1}$,
% a possible high level plan with a feasible refinement for achieving $\G$ is
% \begin{tightlist}
% \item[1.] \emph{MoveTo}$(robot\_init\_pose, rgripper\_bp\_obj_{1})$
% \item[2.] \emph{Grasp}$(obj_{1}, obj_{1}\_init\_pose, rgripper\_bp\_obj_{1},\\rgripper\_gp\_obj_{1}, rgripper)$
% \end{tightlist}

% The $bp$ parameters refer to robot base poses in preparation for grasping an object.
% The trajectory parameters are not included here because they are based on the interface layer's
% sampling of the base pose and grasping pose. If $obj_{1}$ is
% obstructed by, say, $obj_{3}$ in the environment, a possible high level plan would involve
% grasping first $obj_{3}$ then $obj_{1}$.

Classical task planners operate on discrete, purely logical levels and thus cannot directly
solve this problem. We adopt the hierarchical method of Srivastava et al.~\cite{srivastava2014combined}
to account for this discrepancy. A key step in their approach is the
abstraction of continuous state variables, such as robot grasping poses for objects, into a
discrete set of symbolic references to potential values. A \emph{symbolic}, or \emph{high level}, plan
refers to the fixed task sequence returned by a task planner and comprised of these symbolic references.
An \emph{interface layer} assigns continuous values to symbolic references, in a process called \emph{plan refinement}.
These parameters define a sequence of states, and a motion planner is used to find trajectories between these states.
If no satisfying set of parameters can be found (e.g., because an object is obstructed), the interface layer
augments the symbolic fluent state to reflect this and obtains a new high level plan from the task planner.

The authors introduce an algorithm for plan refinement
called \textsc{TryRefine}, an exhaustive backtracking routine over a discrete set of candidate
plan parameter values. (Interested readers are referred to \cite{srivastava2014combined} for details.)
This hand-coded discretization is domain-specific and tuned to the geometry of objects in the
environment. For example, in a pick-and-place task, end effector grasp poses for a can are
sampled around the can in each of the 8 cardinal and intermediate directions, at a fixed distance and height.
Such coarse sampling distributions necessitate fine-tuning for each domain and are not robust to
increased environmental complexity.

An unpublished extension to this system that is currently in progress achieves completeness by
maintaining a \emph{plan refinement graph}, whose nodes each store a valid symbolic plan and
its current set of instantiated parameter values. If refinement of a plan $p$ stored in node
$n$ leads to discovered facts being propagated to the task planner, the newly produced plan $p'$ based on
the updated fluent state is added as a child node $n'$ of $n$. This makes it possible to interleave
partial refinement of several high level plans, in an attempt to find a successful complete refinement
of any one.
% For example, consider the previous pick domain specification where $obj_{3}$ does obstruct $obj_{1}$.
% Initially, the task planner returns a high level plan to immediately grasp $obj_{1}$, because
% it is not yet aware of the obstruction. The interface layer then samples grasp poses
% for $obj_{1}$, but motion planning for each sample fails due to the obstruction. Eventually,
% this error is propagated back to the task planner. The new high level plan correctly
% grasps $obj_{3}$ before $obj_{1}$, and refining this plan succeeds.

% For example, consider a pick domain with two objects: a target $o_{t}$ to be grasped and an obstruction.
% Initially, the task planner returns a high level plan to immediately grasp $o_{t}$, because
% it is not yet aware of the obstruction. The interface layer then samples grasp poses
% for $o_{t}$, but motion planning for each sample fails due to the obstruction. Eventually,
% this error information about the obstruction is converted into a symbolic representation
% and propagated back to the task planner. The new high level plan correctly grasps the obstruction
% before grasping $o_{t}$, and refining this plan succeeds.