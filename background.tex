\section{Background}
We provide relevant technical background and introduce notation
used throughout the paper.

\subsection{Reinforcement Learning}
A reinforcement learning (RL) problem is typically formulated as
a Markov decision process (MDP). An MDP is defined by the following:

\begin{tightlist}
\item A state space $\St$.
\item An action space $\A$.
\item A transition function $P(s' | s, a)$ for $s, s' \in \St, a \in \A$.
\item A reward function $R(s, a, s')$ for $s, s' \in \St, a \in \A$.
\item A discount factor $\gamma \in [0, 1]$.
\end{tightlist}
A solution to an MDP is a policy $\pi: \St \rightarrow \A$ that maps states to
actions. The value $V_{\pi}(s)$ of a state under $\pi$ is the sum of expected
discounted future rewards from starting in state $s$ and selecting actions according
to $\pi$:
$$V_{\pi}(s) = \mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t}) | \pi, s_{0} = s]$$
The optimal policy $\pi^{*}$ maximizes this value for all states. The goal of an RL
problem is for an agent to learn $\pi^{*}$ (or an approximation) automatically
through interaction with its environment.

\subsection{Reinforcement Learning for Planning}
Our problem formulation is motivated by Zhang and Dietterich's application of RL to job
shop scheduling~\cite{JobShopSched}. Job shop scheduling is a combinatorial optimization problem where the goal is to find
a minimum-duration schedule of a set of jobs with temporal and resource constraints. An empirically
successful approach to this problem relies on a randomized local search that proposes changes to an
existing suboptimal schedule. The authors formulate this search as an MDP as follows:
\begin{tightlist}
\item $\St$ is the set of all possible temporal configurations and resource allocations of the jobs.
\item $\A$ is the set of all possible changes to either the time or the resource allocation of a single job.
\item $R$ is a measure of schedule goodness based on its length relative to the difficulty of the scheduling problem.
\end{tightlist}
They use $TD(\lambda)$~\cite{suttonbarto} with function approximation to learn a value function
for this MDP. Their approach outperforms the previous state of the art for this task;
it also scales better to larger scheduling problems after having been trained only on smaller ones.
This application of RL to planning has been built on extensively
in the literature, applied to domains such as retailer inventory
management~\cite{van1997neuro}, single-agent manufacturing systems~\cite{Wang200573}, and multi-robot
task allocation~\cite{dahl2009multi}.

Zucker et al.~\cite{workspacebias} use RL to bias the distribution of a rapidly exploring random tree (RRT)
for motion planning. Their approach uses features of a discretization of the workspace to train
a non-uniform configuration space sampler. They use a variant of the \textsc{Reinforce} family
of stochastic policy gradient algorithms to learn a good weight vector for these features.
Their experimental results, which focus on biasing sampling distributions for the bidirectional
RRT algorithm, demonstrate reduced motion planning time for a variety of scenarios involving path
planning through obstructed environments. In our work, we adopt their policy gradient updates for the TAMP framework
(Section V-C).

\subsection{Task and Motion Planning}
We formulate a TAMP problem as follows. A fully observed task planning
problem is a tuple $(\Obj, \F, \I, \G, \C)$:

$\Obj$ is a set of symbolic references to \emph{objects} in the environment,
such as cylinders, cylinder locations, and grasping poses.

$\F$ is a set of fluents, which define relationships among objects and hold either
true or false in the state.

$\I$ is the set of fluents that hold true in the initial state. All others are assumed false.

$\G$ is the set of fluents defining the goal state.

$\C$ is a set of parametrized \emph{high level actions}, defined by \emph{preconditions}, a set
of fluents characterizing what must hold true in the current state for the action
to be validly performed, and \emph{effects}, a set of fluents that hold true after
the action is performed.

A solution to a task planning problem is a valid sequence of actions
$a_{0}, a_{1}, ..., a_{n} \in \C$ which, when applied to the state of fluents
successively starting with $\I$, results in a state in which all fluents of
$\G$ hold true. We refer to this sequence of actions as a \emph{symbolic}, or
\emph{high level}, plan.

% As a simple example, we consider a pick domain where the robot is capable of
% performing only two actions: moving to a location and grasping an object. We can
% then specify the planning problem under our representation for the goal of holding
% a particular object $obj_{1}$ in the robot's right gripper.

% \begin{tightlist}
% \item[$\Obj$:] Objects in the environment $obj_{0}, ..., obj_{n}$, object initial poses,
% robot initial pose, and left gripper and right gripper grasping poses
% for each object. $\Obj$ is a set of symbolic references, represented at the high
% level only as strings with no geometric interpretation.

% \item[$\F$:] \emph{Obstructs}$(traj, obj_{0}, obj_{1})$, \emph{InGripper}$(obj, gripper)$,\\\emph{Empty}$(gripper)$,
% \emph{IsGraspPose}$(pose, obj, gripper)$,\\\emph{At}$(obj, pose)$, \emph{RobotAt}$(pose)$,\\
% \emph{IsValidTraj}$(traj, pose_{0}, pose_{1})$. Here, the \emph{IsValidTraj} predicate checks that $traj$
% joins $pose_{0}$ with $pose_{1}$, and that it is feasible to execute and
% collision-free.

% \item[$\I$:] \emph{IsGraspPose} between all grasping poses and their corresponding object,
% \emph{RobotAt} the robot's initial pose, \emph{At} for all object initial poses, and
% \emph{Empty} for both grippers.

% \item[$\G$:] \emph{InGripper}$(obj_{1}, right\_gripper)$.

% \item[$\C$:]
% \begin{tightlist}
% \item[1)] \emph{MoveTo}
%   \begin{tightlist}
%   \item[params:]$pose_{0}, pose_{1}, traj$
%   \item[preconds:]\emph{RobotAt}$(pose_{0}) \wedge$\\ \emph{IsValidTraj}$(traj, pose_{0}, pose_{1})$
%   \item[effects:]\emph{RobotAt}$(pose_{1}) \wedge \lnot$\emph{RobotAt}$(pose_{0})$
%   \end{tightlist}
% \item[2)] \emph{Grasp}
%   \begin{tightlist}
%   \item[params:]$obj, obj\_pose, robot\_pose,\\grasp\_pose, gripper, traj$
%   \item[preconds:]\emph{RobotAt}$(robot\_pose) \wedge$\\ \emph{Empty}$(gripper) \wedge$
% \emph{At}$(obj, obj\_pose) \wedge$\\ \emph{IsGraspPose}$(grasp\_pose, obj, gripper) \wedge$\\ \emph{IsValidTraj}$(traj, robot\_pose, grasp\_pose)$
%   \item[effects:]\emph{RobotAt}$(grasp\_pose) \wedge$ $\lnot$\emph{RobotAt}$(robot\_pose) \wedge \lnot$\emph{Empty}$(gripper) \wedge\\
% \lnot$\emph{At}$(obj, obj\_pose) \wedge$ \emph{InGripper}$(obj, gripper) \wedge \forall\ traj', o: \lnot$\emph{Obstructs}$(traj', obj, o)$
%   \end{tightlist}
% \end{tightlist}
% \end{tightlist}

% For the \emph{Grasp} action, the fact that the path to $obj$ must be collision-free is
% implicitly checked within the \emph{IsValidTraj} precondition. An important aspect of
% this formulation is the assumption that grasping an object causes it to no longer
% obstruct any other object in the environment. If no other object obstructs $obj_{1}$,
% a possible high level plan with a feasible refinement for achieving $\G$ is
% \begin{tightlist}
% \item[1.] \emph{MoveTo}$(robot\_init\_pose, rgripper\_bp\_obj_{1})$
% \item[2.] \emph{Grasp}$(obj_{1}, obj_{1}\_init\_pose, rgripper\_bp\_obj_{1},\\rgripper\_gp\_obj_{1}, rgripper)$
% \end{tightlist}

% The $bp$ parameters refer to robot base poses in preparation for grasping an object.
% The trajectory parameters are not included here because they are based on the interface layer's
% sampling of the base pose and grasping pose. If $obj_{1}$ is
% obstructed by, say, $obj_{3}$ in the environment, a possible high level plan would involve
% grasping first $obj_{3}$ then $obj_{1}$.

Since task planners operate on a symbolic level,
they are unable to maintain a geometric interpretation for their continuous variables, such
as grasping poses for objects. Thus, we use TAMP to provide this interpretation. We adopt the hierarchical method of
Srivastava et al.~\cite{srivastava2014combined}, a modular approach to TAMP
that uses a black-box task planner and motion planner. A key step in their approach is the
abstraction of continuous state variables into a discrete set of symbolic references to
potential values. An \emph{interface layer} is then responsible
for assigning continuous values to instantiate these symbolic references, discovering geometric facts about
the environment, and sending these facts back to the task planner to be added into the fluent state.
\emph{Plan refinement} refers to the process of attempting to assign a feasible set of
values to the symbolic plan variables.

The authors introduce an algorithm for plan refinement
called \textsc{TryRefine}, an exhaustive backtracking routine over a hand-coded discrete set of candidate
plan parameter values. (Interested readers are referred to \cite{srivastava2014combined} for details.)
These hand-coded discretizations are domain-specific and tuned to the geometry of objects in the
environment; for example, in a pick-and-place domain, end effector grasp poses for a can may be
sampled around the can in each of the 4 cardinal directions, at a fixed distance and height.
Such sampling distributions necessitate fine-tuning for each domain separately and are not robust to
increased environmental complexity.
% For example, consider the previous pick domain specification where $obj_{3}$ does obstruct $obj_{1}$.
% Initially, the task planner returns a high level plan to immediately grasp $obj_{1}$, because
% it is not yet aware of the obstruction. The interface layer then samples grasp poses
% for $obj_{1}$, but motion planning for each sample fails due to the obstruction. Eventually,
% this error is propagated back to the task planner. The new high level plan correctly
% grasps $obj_{3}$ before $obj_{1}$, and refining this plan succeeds.

% For example, consider a pick domain with two objects: a target $o_{t}$ to be grasped and an obstruction.
% Initially, the task planner returns a high level plan to immediately grasp $o_{t}$, because
% it is not yet aware of the obstruction. The interface layer then samples grasp poses
% for $o_{t}$, but motion planning for each sample fails due to the obstruction. Eventually,
% this error information about the obstruction is converted into a symbolic representation
% and propagated back to the task planner. The new high level plan correctly grasps the obstruction
% before grasping $o_{t}$, and refining this plan succeeds.