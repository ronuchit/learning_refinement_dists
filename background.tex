\section{Background}
We provide relevant technical background and introduce notation
used throughout the paper.

\subsection{Reinforcement Learning for Planning}
A reinforcement learning problem is typically formulated as
a Markov decision process (MDP), defined by a state space
$S$; action space $A$; transition function $P(s' | s, a)$ for $s, s' \in S, a \in A$;
reward function $R(s, a, s')$; and discount factor $\gamma \in [0, 1]$. The goal
is to train a policy that selects which action $a \in A$ to execute given a current
state $s \in S$, in order to maximize the sum of expected rewards. Zhang
and Dietterich~\cite{JobShopSched} present a technique for formulating the job shop scheduling
problem using the reinforcement learning framework. They define the following:
$S$ is all temporal configurations and resource allocations of the jobs; $A$ is changing
either the time or resource allocation of a single job; $R$ is a measure of schedule
goodness based on its length relative to the difficulty of the scheduling problem.
They then use the temporal difference learning algorithm $TD(\lambda)$~\cite{suttonbarto}
to learn a heuristic state evaluation function, used in a one-step lookahead search
to produce good schedules.

\subsection{Task and Motion Planning}
We summarize the work of Srivastava et al.~\cite{srivastava2014combined}. A fully observed task planning
problem is a tuple $(\Obj, \F, \I, \G, \C)$:

$\Obj$ is a set of symbolic references to \emph{objects} in the environment,
such as cylinders, cylinder locations, and grasping poses. (These are treated as strings.)

$\F$ is a set of fluents (which define relationships among objects and hold either
true or false in the state).

$\I$ is the set of fluents that hold true in the initial state
(all others are assumed false).

$\G$ is the set of fluents defining the goal state.

$\C$ is a set of parametrized \emph{high level actions}, defined by \emph{preconditions} (a set
of fluents characterizing what must hold true in the current state for the action
to be validly peformed) and \emph{effects} (a set of fluents that hold true after
the action is performed).

A solution to the task planning problem is then a valid sequence of actions
$a_{0}, a_{1}, ..., a_{n} \in \C$ which, when applied to the state of fluents
successively starting with $\I$, results in a state in which all fluents of
$\G$ hold true. We refer to this sequence of actions as a \emph{symbolic}, or
\emph{high level}, plan: the task planner maintains no knowledge about the geometric meaning
behind any of the symbols.

% As a simple example, we consider a pick domain where the robot is capable of
% performing only two actions: moving to a location and grasping an object. We can
% then specify the planning problem under our representation for the goal of holding
% a particular object $obj_{1}$ in the robot's right gripper.

% \begin{tightlist}
% \item[$\Obj$:] Objects in the environment $obj_{0}, ..., obj_{n}$, object initial poses,
% robot initial pose, and left gripper and right gripper grasping poses
% for each object. $\Obj$ is a set of symbolic references, represented at the high
% level only as strings with no geometric interpretation.

% \item[$\F$:] \emph{Obstructs}$(traj, obj_{0}, obj_{1})$, \emph{InGripper}$(obj, gripper)$,\\\emph{Empty}$(gripper)$,
% \emph{IsGraspPose}$(pose, obj, gripper)$,\\\emph{At}$(obj, pose)$, \emph{RobotAt}$(pose)$,\\
% \emph{IsValidTraj}$(traj, pose_{0}, pose_{1})$. Here, the \emph{IsValidTraj} predicate checks that $traj$
% joins $pose_{0}$ with $pose_{1}$, and that it is feasible to execute and
% collision-free.

% \item[$\I$:] \emph{IsGraspPose} between all grasping poses and their corresponding object,
% \emph{RobotAt} the robot's initial pose, \emph{At} for all object initial poses, and
% \emph{Empty} for both grippers.

% \item[$\G$:] \emph{InGripper}$(obj_{1}, right\_gripper)$.

% \item[$\C$:]
% \begin{tightlist}
% \item[1)] \emph{MoveTo}
%   \begin{tightlist}
%   \item[params:]$pose_{0}, pose_{1}, traj$
%   \item[preconds:]\emph{RobotAt}$(pose_{0}) \wedge$\\ \emph{IsValidTraj}$(traj, pose_{0}, pose_{1})$
%   \item[effects:]\emph{RobotAt}$(pose_{1}) \wedge \lnot$\emph{RobotAt}$(pose_{0})$
%   \end{tightlist}
% \item[2)] \emph{Grasp}
%   \begin{tightlist}
%   \item[params:]$obj, obj\_pose, robot\_pose,\\grasp\_pose, gripper, traj$
%   \item[preconds:]\emph{RobotAt}$(robot\_pose) \wedge$\\ \emph{Empty}$(gripper) \wedge$
% \emph{At}$(obj, obj\_pose) \wedge$\\ \emph{IsGraspPose}$(grasp\_pose, obj, gripper) \wedge$\\ \emph{IsValidTraj}$(traj, robot\_pose, grasp\_pose)$
%   \item[effects:]\emph{RobotAt}$(grasp\_pose) \wedge$ $\lnot$\emph{RobotAt}$(robot\_pose) \wedge \lnot$\emph{Empty}$(gripper) \wedge\\
% \lnot$\emph{At}$(obj, obj\_pose) \wedge$ \emph{InGripper}$(obj, gripper) \wedge \forall\ traj', o: \lnot$\emph{Obstructs}$(traj', obj, o)$
%   \end{tightlist}
% \end{tightlist}
% \end{tightlist}

% For the \emph{Grasp} action, the fact that the path to $obj$ must be collision-free is
% implicitly checked within the \emph{IsValidTraj} precondition. An important aspect of
% this formulation is the assumption that grasping an object causes it to no longer
% obstruct any other object in the environment. If no other object obstructs $obj_{1}$,
% a possible high level plan with a feasible refinement for achieving $\G$ is
% \begin{tightlist}
% \item[1.] \emph{MoveTo}$(robot\_init\_pose, rgripper\_bp\_obj_{1})$
% \item[2.] \emph{Grasp}$(obj_{1}, obj_{1}\_init\_pose, rgripper\_bp\_obj_{1},\\rgripper\_gp\_obj_{1}, rgripper)$
% \end{tightlist}

% The $bp$ parameters refer to robot base poses in preparation for grasping an object.
% The trajectory parameters are not included here because they are based on the interface layer's
% sampling of the base pose and grasping pose. If $obj_{1}$ is
% obstructed by, say, $obj_{3}$ in the environment, a possible high level plan would involve
% grasping first $obj_{3}$ then $obj_{1}$.

We emphasize that any continuous variables of this plan, such as grasping poses for
objects, are abstracted into symbols representing their nature. This allows a
classical planner to produce solution plans without any need to interpret these
symbols. An \emph{interface layer} is then responsible for assigning continuous values to
the high level plan variables, discovering facts about the environment, and sending
these facts back to the task planner to be added into the fluent state. \emph{Refinement} is the
process of assigning such continuous values to the abstract plan variables, grounding the plan. 
% For example, consider the previous pick domain specification where $obj_{3}$ does obstruct $obj_{1}$.
% Initially, the task planner returns a high level plan to immediately grasp $obj_{1}$, because
% it is not yet aware of the obstruction. The interface layer then samples grasp poses
% for $obj_{1}$, but motion planning for each sample fails due to the obstruction. Eventually,
% this error is propagated back to the task planner. The new high level plan correctly
% grasps $obj_{3}$ before $obj_{1}$, and refining this plan succeeds.

For example, consider a pick domain with two objects: a target $o_{t}$ to be grasped and an obstruction.
Initially, the task planner returns a high level plan to immediately grasp $o_{t}$, because
it is not yet aware of the obstruction. The interface layer then samples grasp poses
for $o_{t}$, but motion planning for each sample fails due to the obstruction. Eventually,
this error information about the obstruction is converted into a symbolic representation
and propagated back to the task planner. The new high level plan correctly grasps the obstruction
before grasping $o_{t}$, and refining this plan succeeds.

Srivastava et al.~\cite{srivastava2014combined} introduce a refinement algorithm called \textsc{TryRefine}, an exhaustive backtracking routine for
finding a motion planning feasible set of instantiations of the high level plan
variables. The algorithm progresses by sampling from the refinement
distribution associated with each high level plan variable incrementally; it backtracks
to resample the previous one each time the sample space for a particular variable
is exhausted. The completeness of this backtracking strategy, thus, depends on the refinement distribution
being composed of a discrete and finite set of values. Furthermore, it does not take into consideration
the particular causes of infeasibility at each iteration.