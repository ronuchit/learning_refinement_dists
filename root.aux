\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\bibstyle{IEEEtran}
\citation{srivastava2014combined}
\citation{kaelbling2011hierarchical}
\citation{lagriffoul2014orientation}
\citation{GarrettWAFR14}
\citation{dornhege2012semantic}
\citation{srivastava2014combined}
\citation{JobShopSched}
\citation{workspacebias}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  {Screenshots showing distributions learned by our system in a simulated pick-and-place domain. We use reinforcement learning to train good sampling distributions for continuous motion planning parameters in long-horizon tasks. The robot must grasp the black can and put it down on the red square. The left image shows learned base position (blue) and grasping (green) distributions, and the right shows learned base position (blue) and putdown (green) distributions. The grasping policy learned to avoid the obstructions.}\relax }}{1}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:cover}{{1}{1}{\small {Screenshots showing distributions learned by our system in a simulated pick-and-place domain. We use reinforcement learning to train good sampling distributions for continuous motion planning parameters in long-horizon tasks. The robot must grasp the black can and put it down on the red square. The left image shows learned base position (blue) and grasping (green) distributions, and the right shows learned base position (blue) and putdown (green) distributions. The grasping policy learned to avoid the obstructions.}\relax \relax }{figure.caption.1}{}}
\citation{srivastava2014combined}
\citation{kaelbling2011hierarchical}
\citation{lagriffoul2014orientation}
\citation{GarrettWAFR14}
\citation{srivastava2014combined}
\citation{srivastava2014combined}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Formalization}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-A}Task Planning}{2}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-B}Task and Motion Planning[TO DO SID]}{2}{subsection.3.2}}
\citation{suttonbarto}
\citation{JobShopSched}
\citation{suttonbarto}
\citation{workspacebias}
\citation{JobShopSched}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-C}Markov Decision Processes and Reinforcement Learning}{3}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-D}Reinforcement Learning for Planning}{3}{subsection.3.4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Randomized refinement.\relax }}{3}{algorithm.1}}
\newlabel{alg-randref}{{1}{3}{Randomized refinement.\relax \relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Overall Algorithm[TO DO SID]}{3}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-A}Plan Refinement Graph[TO DO SID]}{3}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-B}Proof of Completeness[TO DO SID]}{3}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-C}Guided Search Techniques[TO DO SID]}{3}{subsection.4.3}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Randomized Refinement}{3}{section.5}}
\citation{workspacebias}
\citation{workspacebias}
\citation{chib1995understanding}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Learning Refinement Distributions}{4}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VI-A}Formulation as Reinforcement Learning Problem}{4}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VI-B}Training Process}{4}{subsection.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VI-C}Distribution and Gradient Updates}{4}{subsection.6.3}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Selecting a Plan to Try Refining}{4}{section.7}}
\citation{srivastava2014combined}
\@writefile{toc}{\contentsline {section}{\numberline {VIII}Experiments}{5}{section.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VIII-A}Training Methodology}{5}{subsection.8.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  {Learned base position (blue) and left arm grasp (green) distributions used to pick up the black can after different training iterations for learning refinement policies. An iteration refers to a single planning problem, which terminates after $L$ calls to the \textsc  {resample} routine. The initial distributions are uniform because we initialize weights to $\mathaccentV {vec}17E{\mathbf  {0}}$. The final distributions are after 12 iterations.}\relax }}{5}{figure.caption.2}}
\newlabel{fig:training}{{2}{5}{\small {Learned base position (blue) and left arm grasp (green) distributions used to pick up the black can after different training iterations for learning refinement policies. An iteration refers to a single planning problem, which terminates after $L$ calls to the \textsc {resample} routine. The initial distributions are uniform because we initialize weights to $\vec {\mathbf {0}}$. The final distributions are after 12 iterations.}\relax \relax }{figure.caption.2}{}}
\citation{Diankov_2008_6117}
\citation{schulman2013finding}
\citation{FF}
\bibdata{$LATEXROOT/references}
\bibcite{srivastava2014combined}{1}
\bibcite{kaelbling2011hierarchical}{2}
\bibcite{lagriffoul2014orientation}{3}
\bibcite{GarrettWAFR14}{4}
\bibcite{dornhege2012semantic}{5}
\bibcite{JobShopSched}{6}
\bibcite{workspacebias}{7}
\bibcite{suttonbarto}{8}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces \relax \fontsize  {9}{10pt}\selectfont  {Percent solved and standard deviation, along with time spent motion planning and number of calls to the motion planner for the baseline system (B), our learned refinement policies with depth-first search through the plan refinement graph (L), and our learned refinement policies and heuristics for searching the graph (F). Results for L and F are averaged across 10 separately trained sets of weights. Time limit: 300s.}\relax }}{6}{table.caption.3}}
\newlabel{table:results}{{I}{6}{\small {Percent solved and standard deviation, along with time spent motion planning and number of calls to the motion planner for the baseline system (B), our learned refinement policies with depth-first search through the plan refinement graph (L), and our learned refinement policies and heuristics for searching the graph (F). Results for L and F are averaged across 10 separately trained sets of weights. Time limit: 300s.}\relax \relax }{table.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VIII-B}Can Domain}{6}{subsection.8.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VIII-C}Dinner Domain}{6}{subsection.8.3}}
\@writefile{toc}{\contentsline {section}{\numberline {IX}Conclusion}{6}{section.9}}
\@writefile{toc}{\contentsline {section}{References}{6}{section*.4}}
\bibcite{chib1995understanding}{9}
\bibcite{Diankov_2008_6117}{10}
\bibcite{schulman2013finding}{11}
\bibcite{FF}{12}
